{"componentChunkName":"component---src-pages-ibmcloud-vpc-blockstorage-index-mdx","path":"/ibmcloud/vpc-blockstorage/","result":{"pageContext":{"frontmatter":{"title":"Sharing Block Storage Volumes Across Availability Zones in VPC in IBM Cloud"},"relativePagePath":"/ibmcloud/vpc-blockstorage/index.mdx","titleType":"page","MdxNode":{"id":"31ac2544-02b9-5c01-b313-16d1b4a969a1","children":[],"parent":"0469d15c-b764-59ce-a64f-160857f0f148","internal":{"content":"---\ntitle: Sharing Block Storage Volumes Across Availability Zones in VPC in IBM Cloud\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\n\n</PageDescription>\n\n## Introduction\nThere might be situations where customer scenario would require sharing of tiered block storage volumes of varied IOPS to be shared across computes in different availability zones. This article leverages SSHFS to overcome this limitation using a very basic service of SSH which is availabile across operating systems flavours. This recipe is specific to IBM Cloud VPC network construct.\n\nNote: There currently is not NFS offering in VPC, but one can setup NFS servers and clients and manage them. In this article we are using sshfs instead of NFS to share the volume from one VSI to other VSIs in different regions.\n\n\n## Architecture\nThis recipe is based on Implementing below architecture where in block storage for VPC is mounted on virtual server based out of Dallas 1 zone. This volume is then mounted on virtual servers in zones – Dallas2 and Dallas3. So whatever data is written to Dallas1 volume is replicated to Dallas2 and Dallas3. I will take virtual server hosted in Dallas1 data center as primary server or master, where as servers hosted on Dallas2 and Dallas3 as secondary servers or Slaves.\n\n![keymanagement with openshift-1](images/Screenshot_1.jpg)\n\n\n## Pre-requisites\nTo Implement this scenario one would need to establish passwordless authentication between secondary servers and primary server or master as shown in below diagram. IBM Cloud VPC comes with private key authentication mechanism by default and one can’t establish passwordless authentication that. Hence one would need to change SSH configuration from private key authentication to password based authentication. Install fuse-sshfs library on secondary servers by running below command :\n\n![keymanagement with openshift-1](images/Screenshot_2_0.jpg)\n\nReboot the machine once after installing fuse-sshfs as many times it throws below error during mounting:\n\n## “Transport endpoint is not connected“\n\n or one can manually download the latest verion:\n\nwget https://github.com/libfuse/sshfs/releases/download/sshfs-3.5.2/sshfs-3.5.2.tar.xz\n\n![keymanagement with openshift-1](images/Screenshot_3.jpg)\n\nThe details of virtual Instances are mapped across availability zones as below:\n\n![keymanagement with openshift-1](images/screenshot3.jpg)\n\n## Attach Block Storage Volume to Primary Server\n\n![keymanagement with openshift-1](images/Screenshot_5.jpg)\n\n![keymanagement with openshift-1](images/Screenshot_6.jpg)\n\nVerify in Instance that volume is attached.\n\n![keymanagement with openshift-1](images/Screenshot_7.jpg)\n\n## Create Volume on Attached Storage\n\nVerify that disk is mounted in OS\n\n![keymanagement with openshift-1](images/Screenshot_8.jpg)\n\nCreate Volume that we will mount on other two Instances spread across zones.\n\n![keymanagement with openshift-1](images/Screenshot_9.jpg)\n\n![keymanagement with openshift-1](images/Screenshot_10.jpg)\n\nSet xfs file system on the partition.\n\n![keymanagement with openshift-1](images/Screenshot_11.jpg)\n\nMount the partition to Posix compliant directory.\n\n![keymanagement with openshift-1](images/Screenshot_12.jpg)\n\nMake entry in /etc/fstab to make mount permanent.\n\n![keymanagement with openshift-1](images/Screenshot_13.jpg)\n\nReboot the virtual server.\n\n![keymanagement with openshift-1](images/Screenshot_14.jpg)\n\n## Passwordless Authentication Between Master Server and Slaves Servers\nMake entry in /etc/hosts of virtual servers in az2 and az3 for virtual server in az1.\n\n![keymanagement with openshift-1](images/Screenshot_15.jpg)\n\nEnable passwordless authentication between virtual server on az2, az3 – Secondary and that of az1 – Primary.\n\n![keymanagement with openshift-1](images/Screenshot_16.jpg)\n\n![keymanagement with openshift-1](images/Screenshot_17.jpg)\n\nThis establishes the passwordless SSH authentication between secondary and primary server.\n\n## Mount Remote File System of Primary Instance on Secondary Instace\nWe have now reached the stage where we need to mount volume that we provisioned in Primary instance in availability zone 1 named as vs1-az1-instance on storage of Secondary instances – vs1-az2-instance and vs1-az3-instance.\n\nThe command that I will use for mounting is  –\n\nsshfs root@vs1-az1-instance:/shareddata <<Mount_Point>>\n\nwhere mount points are :\n\na) /root/vs1-az2-mount – Availability Zone 2\n\nb) /root/vs1-az3-mount – Availability Zone 3\n\n### vs1-az2-instance\n![keymanagement with openshift-1](images/Screenshot_18.jpg)\n\nMake mount permanent by making entry in /etc/fstab\n\n### Note: To auto-remount on primary server reboot make below entries in your /etc/fstab.\n\n![keymanagement with openshift-1](images/Screenshot_19.jpg)\n\n### root@vs1-az1-instance:/shareddata /root/vs1-az2-mount fuse.sshfs allow_other,_netdev,noatime,reconnect,auto 0 0\n\nWhich if not there would not remount the volume in case of primary server reboot and one would need to reboot secondary servers to make that happen.\n\nReboot machine and verify if mount still persists.\n\n![keymanagement with openshift-1](images/Screenshot_20.jpg)\n\n### vs1-az3-instance\n![keymanagement with openshift-1](images/Screenshot_21.jpg)\n\n![keymanagement with openshift-1](images/Screenshot_22.jpg)\n\nWe are now ready for testing the solution.\n\n## Testing the Solution\nThe virtual servers and storage that we configured above are as below:\n\n![keymanagement with openshift-1](images/Screenshot_23.jpg)\n\n## Conclusion\nUsing SSHFS is a no cost solution to synchronize the data across virtual servers located in different availability zones.\n\n## References\n\na) https://www.redhat.com/sysadmin/sshfs\n\nb) https://stackoverflow.com/questions/19971811/sshfs-as-regular-user-through-fstab#:~:text=Using%20option%20allow_other%20in%20%2Fetc%2Ffstab%20allows%20other%20users,users%20than%20root%20can%20access%20to%20mount%20point.\n\nc) https://unix.stackexchange.com/questions/14143/what-is-a-better-way-to-deal-with-server-disconnects-of-sshfs-mounts\n\nd) https://linuxize.com/post/how-to-use-sshfs-to-mount-remote-directories-over-ssh/\n\ne) https://jamesnbr.wordpress.com/2019/10/24/install-sshfs-3-5-2-on-centos-8-rhel-8/\n\n\n","type":"Mdx","contentDigest":"23501994c591284c8222bdd6bbcc1b6b","counter":927,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Sharing Block Storage Volumes Across Availability Zones in VPC in IBM Cloud"},"exports":{},"rawBody":"---\ntitle: Sharing Block Storage Volumes Across Availability Zones in VPC in IBM Cloud\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\n\n</PageDescription>\n\n## Introduction\nThere might be situations where customer scenario would require sharing of tiered block storage volumes of varied IOPS to be shared across computes in different availability zones. This article leverages SSHFS to overcome this limitation using a very basic service of SSH which is availabile across operating systems flavours. This recipe is specific to IBM Cloud VPC network construct.\n\nNote: There currently is not NFS offering in VPC, but one can setup NFS servers and clients and manage them. In this article we are using sshfs instead of NFS to share the volume from one VSI to other VSIs in different regions.\n\n\n## Architecture\nThis recipe is based on Implementing below architecture where in block storage for VPC is mounted on virtual server based out of Dallas 1 zone. This volume is then mounted on virtual servers in zones – Dallas2 and Dallas3. So whatever data is written to Dallas1 volume is replicated to Dallas2 and Dallas3. I will take virtual server hosted in Dallas1 data center as primary server or master, where as servers hosted on Dallas2 and Dallas3 as secondary servers or Slaves.\n\n![keymanagement with openshift-1](images/Screenshot_1.jpg)\n\n\n## Pre-requisites\nTo Implement this scenario one would need to establish passwordless authentication between secondary servers and primary server or master as shown in below diagram. IBM Cloud VPC comes with private key authentication mechanism by default and one can’t establish passwordless authentication that. Hence one would need to change SSH configuration from private key authentication to password based authentication. Install fuse-sshfs library on secondary servers by running below command :\n\n![keymanagement with openshift-1](images/Screenshot_2_0.jpg)\n\nReboot the machine once after installing fuse-sshfs as many times it throws below error during mounting:\n\n## “Transport endpoint is not connected“\n\n or one can manually download the latest verion:\n\nwget https://github.com/libfuse/sshfs/releases/download/sshfs-3.5.2/sshfs-3.5.2.tar.xz\n\n![keymanagement with openshift-1](images/Screenshot_3.jpg)\n\nThe details of virtual Instances are mapped across availability zones as below:\n\n![keymanagement with openshift-1](images/screenshot3.jpg)\n\n## Attach Block Storage Volume to Primary Server\n\n![keymanagement with openshift-1](images/Screenshot_5.jpg)\n\n![keymanagement with openshift-1](images/Screenshot_6.jpg)\n\nVerify in Instance that volume is attached.\n\n![keymanagement with openshift-1](images/Screenshot_7.jpg)\n\n## Create Volume on Attached Storage\n\nVerify that disk is mounted in OS\n\n![keymanagement with openshift-1](images/Screenshot_8.jpg)\n\nCreate Volume that we will mount on other two Instances spread across zones.\n\n![keymanagement with openshift-1](images/Screenshot_9.jpg)\n\n![keymanagement with openshift-1](images/Screenshot_10.jpg)\n\nSet xfs file system on the partition.\n\n![keymanagement with openshift-1](images/Screenshot_11.jpg)\n\nMount the partition to Posix compliant directory.\n\n![keymanagement with openshift-1](images/Screenshot_12.jpg)\n\nMake entry in /etc/fstab to make mount permanent.\n\n![keymanagement with openshift-1](images/Screenshot_13.jpg)\n\nReboot the virtual server.\n\n![keymanagement with openshift-1](images/Screenshot_14.jpg)\n\n## Passwordless Authentication Between Master Server and Slaves Servers\nMake entry in /etc/hosts of virtual servers in az2 and az3 for virtual server in az1.\n\n![keymanagement with openshift-1](images/Screenshot_15.jpg)\n\nEnable passwordless authentication between virtual server on az2, az3 – Secondary and that of az1 – Primary.\n\n![keymanagement with openshift-1](images/Screenshot_16.jpg)\n\n![keymanagement with openshift-1](images/Screenshot_17.jpg)\n\nThis establishes the passwordless SSH authentication between secondary and primary server.\n\n## Mount Remote File System of Primary Instance on Secondary Instace\nWe have now reached the stage where we need to mount volume that we provisioned in Primary instance in availability zone 1 named as vs1-az1-instance on storage of Secondary instances – vs1-az2-instance and vs1-az3-instance.\n\nThe command that I will use for mounting is  –\n\nsshfs root@vs1-az1-instance:/shareddata <<Mount_Point>>\n\nwhere mount points are :\n\na) /root/vs1-az2-mount – Availability Zone 2\n\nb) /root/vs1-az3-mount – Availability Zone 3\n\n### vs1-az2-instance\n![keymanagement with openshift-1](images/Screenshot_18.jpg)\n\nMake mount permanent by making entry in /etc/fstab\n\n### Note: To auto-remount on primary server reboot make below entries in your /etc/fstab.\n\n![keymanagement with openshift-1](images/Screenshot_19.jpg)\n\n### root@vs1-az1-instance:/shareddata /root/vs1-az2-mount fuse.sshfs allow_other,_netdev,noatime,reconnect,auto 0 0\n\nWhich if not there would not remount the volume in case of primary server reboot and one would need to reboot secondary servers to make that happen.\n\nReboot machine and verify if mount still persists.\n\n![keymanagement with openshift-1](images/Screenshot_20.jpg)\n\n### vs1-az3-instance\n![keymanagement with openshift-1](images/Screenshot_21.jpg)\n\n![keymanagement with openshift-1](images/Screenshot_22.jpg)\n\nWe are now ready for testing the solution.\n\n## Testing the Solution\nThe virtual servers and storage that we configured above are as below:\n\n![keymanagement with openshift-1](images/Screenshot_23.jpg)\n\n## Conclusion\nUsing SSHFS is a no cost solution to synchronize the data across virtual servers located in different availability zones.\n\n## References\n\na) https://www.redhat.com/sysadmin/sshfs\n\nb) https://stackoverflow.com/questions/19971811/sshfs-as-regular-user-through-fstab#:~:text=Using%20option%20allow_other%20in%20%2Fetc%2Ffstab%20allows%20other%20users,users%20than%20root%20can%20access%20to%20mount%20point.\n\nc) https://unix.stackexchange.com/questions/14143/what-is-a-better-way-to-deal-with-server-disconnects-of-sshfs-mounts\n\nd) https://linuxize.com/post/how-to-use-sshfs-to-mount-remote-directories-over-ssh/\n\ne) https://jamesnbr.wordpress.com/2019/10/24/install-sshfs-3-5-2-on-centos-8-rhel-8/\n\n\n","fileAbsolutePath":"/home/runner/work/ibm-enterprise-runbooks/ibm-enterprise-runbooks/src/pages/ibmcloud/vpc-blockstorage/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","243517648","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550","817386451"]}