{"componentChunkName":"component---src-pages-ibmcloud-vpc-vpe-index-mdx","path":"/ibmcloud/vpc-vpe/","result":{"pageContext":{"frontmatter":{"title":"Leveraging Virtual Private Endpoint in IBM Cloud VPC to Connect IBM Cloud Object Storage"},"relativePagePath":"/ibmcloud/vpc-vpe/index.mdx","titleType":"page","MdxNode":{"id":"8b8403ab-6afb-5f35-9adc-9b28a526e96b","children":[],"parent":"6702fbab-4418-52f2-8194-733b4b4662db","internal":{"content":"---\ntitle: Leveraging Virtual Private Endpoint in IBM Cloud VPC to Connect IBM Cloud Object Storage\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\n\n</PageDescription>\n\n## Introduction\nIBM Cloud® Virtual Private Endpoints (VPE) for VPC enables you to connect to supported IBM Cloud services from your VPC network by using the IP addresses of your choosing, allocated from a subnet within your VPC.\n\nVPE is an evolution of the private connectivity to IBM Cloud services. VPEs are virtual IP interfaces that are bound to an endpoint gateway created on a per service, or service instance, basis (depending on the service operation model). The endpoint gateway is a virtualized function that scales horizontally, is redundant and highly available, and spans all availability zones of your VPC. Endpoint gateways enable communications from virtual server instances within your VPC and IBM Cloud® service on the private backbone. VPE for VPC gives you the experience of controlling all the private addressing within your cloud.\n\nFor more details refer: https://cloud.ibm.com/docs/vpc?topic=vpc-about-vpe\n\nIn this recipe I will start with a reference architecture start which I will Implement to conform to certain use case.\n\n## Use Cases\nIn this use case I will be sharing the data across availability zones in VPC and DR site using IBM Cloud object storage where in DC is located in Dallas region and DR is located in Washington region. I will mount cloud object storage bucket across regions and within region to actively share the data.\n\n## VPC - DC and DR Architecture\n![keymanagement with openshift-1](images/Arch_Plain.jpg)\n\n## DC Implementation Architecture in IBM Cloud Console\n![keymanagement with openshift-1](images/VPC_Layout.jpg)\n\n## DR Implementation Architecture in IBM Cloud Console\n![keymanagement with openshift-1](images/VPC_DR_Layout.jpg)\n\n## Adding VPE Gateway \n\nIBM Cloud® Virtual Private Endpoint (VPE) for IBM Cloud™ Virtual Private Cloud provides connection points to IBM services on the IBM private network from your VPC network. VPE has multiple benefits like:\n\n    1. Public connectivity is not required and has no public data egress charges.\n    2. Reaches IBM Cloud assets through a private service provider.\n    3. A VPE lives in your network address space, extending your private and multicloud into the IBM Cloud.\n    4. You can apply security through Network Access Control Lists (NACLs).\n    5. The endpoint IP deploys in a customer-defined, virtual network.\n    6. Includes platform integration to VPC – Identity and Access Management (IAM), network ACLs, and tagging.\n    7. Access to new endpoints is achieved through the UI, CLI, and API.\n    8. You can map a new endpoint to an existing service, as well as map to a shared endpoint.\n    9 Integrates with DNS Services.\n\n \n\nThe COS bucket is already set up in US region as multi-region and to share bucket across compute instances in DC and DR. I will not cover the details of configuring COS in this article. To mount these buckets in compute instances across regions over the private network, we have to setup VPE. For this one would need to be aware of end-point URL of COS bucket.\n\n## Step1\n![keymanagement with openshift-1](images/VPE_gateway.jpg)\n\n## Step2\n![keymanagement with openshift-1](images/VPE2.jpg)\n\n## Step3\n![keymanagement with openshift-1](images/VPE3.jpg)\n\n## Step4\n![keymanagement with openshift-1](images/VPE4.jpg)\n\n## Step5\n![keymanagement with openshift-1](images/VPE5.jpg)\n\nThis will create VPE for our DR site pointing to COS as shown below:\n![keymanagement with openshift-1](images/VPE6.jpg)\n\nSimilar VPE is created for DC Site. As you can see in below screen shot this VPE is linked to two subnets in primary VPC so that bucket can be shared across Instances in these two subnets.\n![keymanagement with openshift-1](images/VPE7.jpg)\n\nVPE uses direct end-points as shown in the above screen shot. This service endpoint can be verified from the bucket Instance. Lets browse through the COS bucket Instance that I have created and understand on how to get the service end-point of the bucket to be used with VPE.\n\n![keymanagement with openshift-1](images/COS_0.jpg)\n\nSince I have created Cross region COS, I will select the region where my bucket is located which is US in current scenario.\n![keymanagement with openshift-1](images/COS_5.jpg)\n\nSince in the scenario DC is in Dallas and DR is Washington, select and copy direct end-point URL for both and keep it for reference\n![keymanagement with openshift-1](images/COS_6.jpg)\n\n## Architecture after VPE is Created\nAfter the VPE is created, we will have a direct private end-point for COS bucket. We are now going to mount this bucket on POSIX file system dirctory in Linux as shown below. The below architetcure depicts accessibility of bucket through VPE.\n![keymanagement with openshift-1](images/COS_9.jpg)   -- Not Working\n\n## Mounting Bucket in Virtual Servers on DC and DR Sites\nNow we have reached the final step of the article where I will be mounting the bucket into Virtual servers in DC spanned across subnets and DR witin one subnet. Before we start with mounting few pre-requisite libraries would need to be Installed.\n\n ![keymanagement with openshift-1](images/COS_1.jpg) \n\n![keymanagement with openshift-1](images/COS_2.jpg) \n\nOnce these are Installed we need to create a credential file in home directory with credentials in the format as below:\n\n![keymanagement with openshift-1](images/COS_7.jpg) \n\n![keymanagement with openshift-1](images/COS_8.jpg) \n\nThe ACCESS_KEY_ID key and SECRET_ACCESS_KEY_ID can be found in bucket configuration.\n\n![keymanagement with openshift-1](images/COS_3.jpg) \n\nRun chmod 600 .passwd-s3fs command on the file, else one would get a permission error on runing mount command.\n\n![keymanagement with openshift-1](images/permission_error.jpg) \n\nCreate a directory where you would like to mount bucket which in my case is “/mycosbucket”. One can create this directory through mkdir command.\n\nAfter the above steps run the command to mount the bucket on POSIX directory.\n\n**s3fs my-bucket-4566 /mycosbucket -o passwd_file=~/.passwd-s3fs -o url=https://s3.direct.dal.us.cloud-object-storage.appdomain.cloud**\n\nand browse the mounted directory to verify its content:\n\n![keymanagement with openshift-1](images/command.jpg) \n\nRepeat the above and below steps for other VSI’s in consideration.\n\n## Mounting Bucket permanently \nTo make mount permanent even after VSI reboot, enter below entry in /etc/fstab directory.\n\n**my-bucket-4566 /mycosbucket fuse.s3fs _netdev,allow_other,use_path_request_style,url=https://s3.direct.dal.us.cloud-object-storage.appdomain.cloud 0 0**\n\n![keymanagement with openshift-1](images/fstab.jpg) \n\nReboot the machine to verify persistence of the mounted bucket. Finally the architecture would look like as below.\n\n![keymanagement with openshift-1](images/Arch_ver1.jpg)\n\n## Testing the Configuration\nNow we have mounted COS bucket in DC on VSI across availability zones and in DR VPC in single AZ, lets verify whether files are shared across consistently or not.\n![keymanagement with openshift-1](images/Testing1.jpg) \n\nThis shows data is synchronized across zones and regions which are participating in DC and DR in our solution.\n","type":"Mdx","contentDigest":"fc859d34c725e44de0510064df2bbd1c","owner":"gatsby-plugin-mdx","counter":915},"frontmatter":{"title":"Leveraging Virtual Private Endpoint in IBM Cloud VPC to Connect IBM Cloud Object Storage"},"exports":{},"rawBody":"---\ntitle: Leveraging Virtual Private Endpoint in IBM Cloud VPC to Connect IBM Cloud Object Storage\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\n\n</PageDescription>\n\n## Introduction\nIBM Cloud® Virtual Private Endpoints (VPE) for VPC enables you to connect to supported IBM Cloud services from your VPC network by using the IP addresses of your choosing, allocated from a subnet within your VPC.\n\nVPE is an evolution of the private connectivity to IBM Cloud services. VPEs are virtual IP interfaces that are bound to an endpoint gateway created on a per service, or service instance, basis (depending on the service operation model). The endpoint gateway is a virtualized function that scales horizontally, is redundant and highly available, and spans all availability zones of your VPC. Endpoint gateways enable communications from virtual server instances within your VPC and IBM Cloud® service on the private backbone. VPE for VPC gives you the experience of controlling all the private addressing within your cloud.\n\nFor more details refer: https://cloud.ibm.com/docs/vpc?topic=vpc-about-vpe\n\nIn this recipe I will start with a reference architecture start which I will Implement to conform to certain use case.\n\n## Use Cases\nIn this use case I will be sharing the data across availability zones in VPC and DR site using IBM Cloud object storage where in DC is located in Dallas region and DR is located in Washington region. I will mount cloud object storage bucket across regions and within region to actively share the data.\n\n## VPC - DC and DR Architecture\n![keymanagement with openshift-1](images/Arch_Plain.jpg)\n\n## DC Implementation Architecture in IBM Cloud Console\n![keymanagement with openshift-1](images/VPC_Layout.jpg)\n\n## DR Implementation Architecture in IBM Cloud Console\n![keymanagement with openshift-1](images/VPC_DR_Layout.jpg)\n\n## Adding VPE Gateway \n\nIBM Cloud® Virtual Private Endpoint (VPE) for IBM Cloud™ Virtual Private Cloud provides connection points to IBM services on the IBM private network from your VPC network. VPE has multiple benefits like:\n\n    1. Public connectivity is not required and has no public data egress charges.\n    2. Reaches IBM Cloud assets through a private service provider.\n    3. A VPE lives in your network address space, extending your private and multicloud into the IBM Cloud.\n    4. You can apply security through Network Access Control Lists (NACLs).\n    5. The endpoint IP deploys in a customer-defined, virtual network.\n    6. Includes platform integration to VPC – Identity and Access Management (IAM), network ACLs, and tagging.\n    7. Access to new endpoints is achieved through the UI, CLI, and API.\n    8. You can map a new endpoint to an existing service, as well as map to a shared endpoint.\n    9 Integrates with DNS Services.\n\n \n\nThe COS bucket is already set up in US region as multi-region and to share bucket across compute instances in DC and DR. I will not cover the details of configuring COS in this article. To mount these buckets in compute instances across regions over the private network, we have to setup VPE. For this one would need to be aware of end-point URL of COS bucket.\n\n## Step1\n![keymanagement with openshift-1](images/VPE_gateway.jpg)\n\n## Step2\n![keymanagement with openshift-1](images/VPE2.jpg)\n\n## Step3\n![keymanagement with openshift-1](images/VPE3.jpg)\n\n## Step4\n![keymanagement with openshift-1](images/VPE4.jpg)\n\n## Step5\n![keymanagement with openshift-1](images/VPE5.jpg)\n\nThis will create VPE for our DR site pointing to COS as shown below:\n![keymanagement with openshift-1](images/VPE6.jpg)\n\nSimilar VPE is created for DC Site. As you can see in below screen shot this VPE is linked to two subnets in primary VPC so that bucket can be shared across Instances in these two subnets.\n![keymanagement with openshift-1](images/VPE7.jpg)\n\nVPE uses direct end-points as shown in the above screen shot. This service endpoint can be verified from the bucket Instance. Lets browse through the COS bucket Instance that I have created and understand on how to get the service end-point of the bucket to be used with VPE.\n\n![keymanagement with openshift-1](images/COS_0.jpg)\n\nSince I have created Cross region COS, I will select the region where my bucket is located which is US in current scenario.\n![keymanagement with openshift-1](images/COS_5.jpg)\n\nSince in the scenario DC is in Dallas and DR is Washington, select and copy direct end-point URL for both and keep it for reference\n![keymanagement with openshift-1](images/COS_6.jpg)\n\n## Architecture after VPE is Created\nAfter the VPE is created, we will have a direct private end-point for COS bucket. We are now going to mount this bucket on POSIX file system dirctory in Linux as shown below. The below architetcure depicts accessibility of bucket through VPE.\n![keymanagement with openshift-1](images/COS_9.jpg)   -- Not Working\n\n## Mounting Bucket in Virtual Servers on DC and DR Sites\nNow we have reached the final step of the article where I will be mounting the bucket into Virtual servers in DC spanned across subnets and DR witin one subnet. Before we start with mounting few pre-requisite libraries would need to be Installed.\n\n ![keymanagement with openshift-1](images/COS_1.jpg) \n\n![keymanagement with openshift-1](images/COS_2.jpg) \n\nOnce these are Installed we need to create a credential file in home directory with credentials in the format as below:\n\n![keymanagement with openshift-1](images/COS_7.jpg) \n\n![keymanagement with openshift-1](images/COS_8.jpg) \n\nThe ACCESS_KEY_ID key and SECRET_ACCESS_KEY_ID can be found in bucket configuration.\n\n![keymanagement with openshift-1](images/COS_3.jpg) \n\nRun chmod 600 .passwd-s3fs command on the file, else one would get a permission error on runing mount command.\n\n![keymanagement with openshift-1](images/permission_error.jpg) \n\nCreate a directory where you would like to mount bucket which in my case is “/mycosbucket”. One can create this directory through mkdir command.\n\nAfter the above steps run the command to mount the bucket on POSIX directory.\n\n**s3fs my-bucket-4566 /mycosbucket -o passwd_file=~/.passwd-s3fs -o url=https://s3.direct.dal.us.cloud-object-storage.appdomain.cloud**\n\nand browse the mounted directory to verify its content:\n\n![keymanagement with openshift-1](images/command.jpg) \n\nRepeat the above and below steps for other VSI’s in consideration.\n\n## Mounting Bucket permanently \nTo make mount permanent even after VSI reboot, enter below entry in /etc/fstab directory.\n\n**my-bucket-4566 /mycosbucket fuse.s3fs _netdev,allow_other,use_path_request_style,url=https://s3.direct.dal.us.cloud-object-storage.appdomain.cloud 0 0**\n\n![keymanagement with openshift-1](images/fstab.jpg) \n\nReboot the machine to verify persistence of the mounted bucket. Finally the architecture would look like as below.\n\n![keymanagement with openshift-1](images/Arch_ver1.jpg)\n\n## Testing the Configuration\nNow we have mounted COS bucket in DC on VSI across availability zones and in DR VPC in single AZ, lets verify whether files are shared across consistently or not.\n![keymanagement with openshift-1](images/Testing1.jpg) \n\nThis shows data is synchronized across zones and regions which are participating in DC and DR in our solution.\n","fileAbsolutePath":"/home/runner/work/ibm-enterprise-runbooks/ibm-enterprise-runbooks/src/pages/ibmcloud/vpc-vpe/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","243517648","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550","817386451"]}