{"componentChunkName":"component---src-pages-install-automation-index-mdx","path":"/install/automation/","result":{"pageContext":{"frontmatter":{"title":"CP4 Automation"},"relativePagePath":"/install/automation/index.mdx","titleType":"page","MdxNode":{"id":"40cc28a9-e887-56df-be30-20bc488452c8","children":[],"parent":"2e4b9351-e245-5b2f-84a3-890e27adb6a2","internal":{"content":"---\ntitle: CP4 Automation\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\n</PageDescription>\n\n## **Automation overview**\n\nDigital business automation (DBA) allows an organization to improve its operations by streamlining the way people participate in business processes and workflows, automate repeatable decisions, and provide business users with the ability to edit and change the business logic involved in these business processes. DBA projects also aim to make documents easy to store and retrieve, digitize document content, such as with optical character recognition (OCR), and automate data entries with software robots, also referred to as robotic process automation.\n\nThe IBM Cloud Pak for Automation offers a software platform to develop, deploy, run and manage your digital business automation projects, using the capabilities shown in the following digram: \n\n## **Receipe Overview**\n\nThe intent of this recipe is to provide a simplified set to steps, to set up an instance for PoC or Demos with least effort. Viewer is advised to refer to the original Enterprise install steps once before going ahead with these steps.\nIngredients\n\n\n1. IBM ROKS Openshift cluster v4.3  (  6 worker nodes with 8 cores x 16 GB RAM )\n\n2. A RHEL 7.x VM ( 4 cores x 16 GB RAM) for  DB2, IBM security Directory server and NFS file share.\n\n3. Cloud Pak for Automation v20.0.1\n\n4. DB2 v11.1\n\n5. IBM Security Directory Server v6.4\n\n6. workstation with oc, kubectl, git  installed for executing the steps during install.\n\n## **Step-by-step**\n\n### **1. Provision Openshift Cluster**\n\nProvision an Openshift cluster v4.3 with 6 worker nodes having 8 cores x 16 GB RAM , on IBM Cloud. If it’s provisioned by someone else, the person following the instructions, should have Admin access to the cluster.  \n\n### **2.Provision 1 or more VMs for shared services. ( 1 would suffice)**\n\nSome of the install instructions related to DB2 and TDS (Tivoli Directory Server , which is now called – IBM Security Directory server )   require use of RPM so one of the supported linux distros is recommended. Steps have been tested with RHEL 7.x.  ( 4 cores x 16 GB RAM) . 4 x 8 should also suffice.\n\n### **3. Clone the github repository for helper scripts**\n\n1.The original (official)  documentation can be found in Knowledge Center. It refers to github repo:  https://github.com/icp4a/cert-kubernetes/tree/20.0.2 for helper scripts.\n\n2. Some additional helper scripts (with some assumptions for Enterprise installs for dev environments ) have been added in this fork of the official project here:  https://github.com/sachinjha/cert-kubernetes/tree/20.0.2\n\n3. Going through the knowledge Center link above is recommended to understand the overall install plan. In knowledge Center, refer to “Enterprise Install” options and anything that applies to “ROKS” ( Redhat Openshift Kubernetes Service , on IBM Cloud ) , as that is our environment.  Instructions which apply to “OCP” but are not supported on ROKS are not applicable. Look for correspoding options for ROKS.\n     \n### **4.Setup NFS services**\n1. The storage capacity in default disk attached to VMs may not be sufficient. (Default is 25GB and max is 100GB). It’s recommended to add an additional disk (File Storage – 750 GB or more)  from IBM cloud console and use that for NFS.\n        \n2. Once you have added disk run – ‘fdisk -l’ to view the disks attached. See image below for example. here /dev/xvdc is the additional device.\n\n3. ![IBM Cloud Automation NFS](images/Automation-NFs.png)\n\n4. Run folowing commands to create filesystem and mount the disk.\n    - mkfs.ext4 /dev/xvdc\n    - mkdir /opt/nfs2\n    - mount /dev/xvdc /opt/nfs2\n        \n5. Follow the steps [here](https://linuxconfig.org/quick-nfs-server-configuration-on-redhat-7-linux) for setting up NFS service on RHEL\n\n6. Perform following steps to configure the folders required for ICP4Auto components and for updating /etc/exports for NFS\n    1. copy files ( configure-nfs.sh  and nfs-exports-configuration.txt)  under this link , in the git project to NFS server.\n    \n    2. configure-nfs.sh will create all the folders with required permissions for components which need NFS based storage.\n    \n    3. nfs-exports-configuration.txt  has the place holders for /etc/exports.\n    \n    4. Replace the W*Public and W*Private  in nfs-exports-configuration.tx with Public and Private IPs of 4 worker nodes. \n   \n    5. chmod +x configure-nfs.sh\n   \n    6. ./configure-nfs.sh\n   \n    7. Verify that  nfs status is green and  /etc/exports is updated with configuration\n \n### **5. Setup other shared Services - LDAP and DB2**\n\n\n####  LDAP and DB2 installation:\n1. Follow the steps on this page for installing DB2 and LDAP.  Instructions are for Linux only. Read the complete instructions for the section before proceeding to perform the steps. Some troubleshooting steps for DB2 install can be found [here](https://github.com/sachinjha/cert-kubernetes/tree/20.0.2/shared-services/db2.md).    \n2. Run the following commands after Create Instance Step in SDS setup, before starting the server. In case you have started it, you can stop it, using the  commands given in the link, peform the steps below and then start it again.\n\n\n\n```sh\n# Configure a database for a directory server instance.\n./idscfgdb -I dsinst1 -a dsinst1 -w <your-password> -t dsinst1 -l /home/dsinst1\n\n# Set the administration DN and administrative password for an instance\n./idsdnpw -I dsinst1 –u cn=root –p <your-password>\n\n# Add suffix\n./idscfgsuf -I dsinst1 -s o=IBM,c=US\n```\n\nUse the LDIF file [referenced here](https://cloudpak8s.io/automation/shared-services/#import-ldap-users-and-groups) to add some default users to LDAP.\n\nNote: One can change the suffix to suit their org and country for e.g. o=ABC,c=IN but make sure to update the references in LDIF file given in the above link,  before importing it into LDAP and also in the CR yaml (icp4acluster) used for installing the components.\n\n \n\n#### 2nd instance of LDAP ( TDS )\n2nd instance of LDAP ( TDS ) is  required for BAN and FNCM if you want external users to access. Steps to create 2nd instance are [here](https://github.com/sachinjha/cert-kubernetes/blob/20.0.1/shared-services/tds%20ext%20ldap%20creation.sh).\n\n\n#### Security Groups for LDAP and DB server \nMake sure the security group attached to the VM allows inbound on port:  22(ssh),  50000 (db2) , 389 (ldap) , 390 ( ext ldap) , 2049( nfs)   and outbound on ports :  53  (dns) , 443 ( https) , 80 ( http) )\n\n### **6. Identify the suffix for Openshift routes and secure routes**\n\n**Use the following option if routes with self signed certificates are not a problem.**\n\n1. Get public IP of loadbalancer using any of the methods given in the image below.\n    \n2. ![IBM Cloud openshift routes](images/Automation-Routes.png)\n\n3. Use a suffix like  publicIP.nip.io\n\n4. For example, in this case ,  suffix can be “169.44.184.38.nip.io”  and a sample hostname for UMS service, can be “ums.169.44.184.38.nip.io”\n     \n**In case there is a need to create secure routes with certificates signed by known CAs then follow the steps below: ( This hasn’t been completely tested for all components )**\n        \n1. ***Update on 31st Aug 2020- secure routes not getting created as expected, with Let’s Encrypt intermediate_cert, so one may choose the above option, or stop at 7 and use the registered domain, allowing operator to generate the certificates.***\n2. In IBM Cloud ->Classic Infrastruction -> Services -> Domain Registration, register a domain  ( e.g. automation.org.xyz.com )  \n3. Create an instance of  \"IBM Internet services\", on IBM cloud  and update the custom name servers in “Domain Registration”  to the name servers given in “IBM Internet Services” setup page.\n4. ![IBM Cloud Automation NFS](images/internet-services.png)\n5. ![IBM Cloud Automation NFS](images/domain-reg.png)\n\n6. Updation of records might take 24 hrs to reflect in \"internet services\"\n7. In \"internet services\" Create a wildcard subdomain  A record  * which maps to the above loadbalancer IP. e.g. *.cp-automation -> < public IP found above >\n8. Create an instance of \"certificate manager\" and select wild card subdomain as CN for certificates.\n9. For Example : Generate an SSL certificate with wild card subdomain ( *.cp-automation.org.xyz.com ) \n10.Let's encrypt certificate should give you key, cert and Intermediate Cert ( which is authorized to issue certs, and can be used as CA) . We will use this to create a secret which will act as the root-ca-secret and will be referred in the Cloud Pak for Automation Custom resource file\n\n### **7. Next steps - Follow Knowledge Center**\n\nSimplified steps are given below i.e. Step 8 onwards  but its important to understand the process as documented in the  Knowledge Center .\n\n1. On ROKS, you need to install cluster manually. \n\n3. Look at Preparing to install Containers for components of interest.\n\n    Foundation pattern is a prerequisite for any other pattern, so start with that and then add different patterns to the custom resource file as required.\n\n4. Deploy custom resource after manually updating the sections for different components with values that need to be configured.\n\n### **8. Create DB2 databases required for different components**\n\nSteps for components which are not of interest can be skipped. UMS should be done as UMS is required for all other components.\n\nExecute all the scripts as user db2inst1. On the DB server, ensure that db2 is in PATH  by running command below and then follow the instructions below for each of the components:\n```     \nsu db2inst1\n./home/db2inst1/sqllib/db2profile\n```\n     \n1. **UMS**\n    - copy [file](https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/UMS/configuration/create-db.sh) or contents of the file and execute them on DB server.\n    \n2. **BACA**\n    - copy [DB2](https://github.com/sachinjha/cert-kubernetes/tree/20.0.2/ACA/configuration-ha/DB2) folder to DB server.\n    - Execute the scripts CreateBaseDB.sh  and AddTenant.sh.\n    - See [log](https://github.com/sachinjha/cert-kubernetes/tree/20.0.1/ACA/configuration-ha/db-creation-logs) files for sample values that can be used. ( mostly defaults )\n        ( bacaadmin is a user created in LDAP when the LDIF file is imported. If you change users, use it accordingly when executing the script)\n\n3. **ODM**\n    - copy [file](https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/ODM/configuration/create-db.sh) or contents of the file and execute them on DB server.\n     \n4. **ICN**\n    \n    - copy [file](https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/BAN/configuration/create-db.sql)   or contents of the file and execute them on DB server\n\n    ```\n    db2 -tvf create-db.sql\n    ```\n\n5. **FNCM**\n    - copy [DB2](https://github.com/sachinjha/cert-kubernetes/tree/20.0.2/FNCM/configuration/DB2) folder to DB server.\n    - Execute the scripts\n        \n    ```\n    ./create-gcd.sh \n    ./create-os.sh\n    ```\n\n6. **APPENG**\n\n    - copy [file](https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/BAS/configuration/appenng-db2.sql) or contents of the file and execute them on DB server\n\n    ```\n    db2 -tvf appeng-db2.sql\n    ```        \n    \n7. **BAStudio**\n\n    - copy [db2](https://github.com/sachinjha/cert-kubernetes/tree/20.0.2/BAS/configuration/db2) folder to DB server.\n    - Use db2 command to execute the .sql files\n\n    ```\n    db2 -tvmf bastudio.sql\n    db2 -tvmf appengine.sql\n    ```\n\n8.  **BAW ( on containers)**\n        \n    - copy file to DB server.\n    - Use db2 command to execute the .sql files\n            \n            \n    ```\n    db2 -tvmf  create-db.sql\n    ```\n\n    - Use the create-os.sh script from FNCM (in step 5) and create 2 Object storage dbs  DOSDB and TOSDB.  Just the db names are different, rest of the values in the script file should be same.\n\n      \n\n\n### **9.Install Operator**\n\n1.\n    ```\n    oc new-project automation.\n    ```\n2. If you use a different name for the project, you may have to edit multiple files in the next step, where namespaces may be hard coded.\n\n3. On ROKS, you need to install cluster manually.  Before creating the operator, look at the\n\n    a) [link for preparing Storage](https://www.ibm.com/support/knowledgecenter/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_prep_operator.html?_ga=2.20303472.936333667.1604297352-1372577732.1601029870) for Operator.  Create storage classes using the sample files given in the project and use one of them for operator-shared-pvc. For e.g. cp4a-file-retain-bronze-gid. Ignore step 6 and 7 at this stage.\n\n    b)  [Getting access to container images](https://developer.ibm.com/recipes/tutorials/access%20to%20container%20images): You can use entitled registry. When creating the secret set docker-username=cp\n\n4. Now create the common service and Operator using the steps given here. If you are going to configure BAI and going to configure IBM Eventstreams for that then execute the create common services script with argument “bai” e.g.\n    \n    ```\n    ./deploy_CS3.4.sh bai\n    ```\n\n5. Now [Install operator](https://www.ibm.com/support/knowledgecenter/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_setup_cluster_manually.html?_ga=2.218958161.936333667.1604297352-1372577732.1601029870)\n6. Copy jdbc jars  to operator pod by following steps [6 and 7 here](https://www.ibm.com/support/knowledgecenter/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_prep_operator.html?_ga=2.20303472.936333667.1604297352-1372577732.1601029870).\n\n\n\n\n\n###  **10.Create secrets , Rolebindings, PVs and PVCs for different components. **\n\nAfter updating the component related files, do not forget to tun the script “create_preequisites.sh” to apply the changes .\n\nPWD = Root folder of cert_kubernetes project.\n\n1.UMS\n    - Update the value of keys – oauthDBPassword and tsDBPassword in UMS/configuration/secrets.yaml with password for db2inst1 set during step 5.\n    - Update the values of keys externalLdapPassword,  ldapPassword and lc_ldap_bind_password to values set during LDAP installation in step 5 in UMS/configuration/ldap-bind-secret.sh\n2.BACA\n    - Update the (file)[https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/ACA/configuration-ha/security/aca-basedb-secret.sh] with db2 password configured in step 5.\n3.ODM\n    - Update the (file)[https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/ODM/configuration/create-db.sh] with db2 password configured in step 5.\n    - If configuring integration with UMS, update (file)[https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/ODM/configuration/security/sample-websecurity-OIDC-ums.xml] with different username or ldap group and Suffix if the default values are not suitable.\n4.BAS \n    - ( Includes Resource Registry and App Engine) Update BAS/configuration/db2/secret.yaml by replacing values of “dbPassword” and “AE_DATABASE_PWD” with password set for db2inst1 in Step 5.\n5.BAI\n    - Update BAI/configuration/bai-psp.yaml and replace CR_NAME with actual custom resource name.\n    - update create-prerequisites.sh and modify icp4adeploy-bai-psp-sa  in “BAI” section according to CR_NAME if it’s different from icp4adeploy.\n6.BAN\n    - Update the values of navigatorDBPassword, ldapPassword , externalLdapPassword in BAN/configuration/ban-secret.sh\n7.FNCM\n    - Update the values of gcdDBPassword, osDBPassword , ldapPassword in FNCM/configuration/ibm-fncm-secret.sh\n    - Update the value of <NFS SERVER> in FNCM/configuration/volumes_cmis.yaml,  graphql_volume.yaml and volumes_cpe.yaml  with with NFS server IP configured in step 4\n8.App Engine:\n    - Update the values of AE_DATABASE_PWD in BAS/configuration/secret.yaml with DB2 password\n9.BAW on containers: \n    - Update the values of db2 passowrd in the file : BAW/configuration/encryption-key-secret.yaml\n\n \n\n ```\ncd scripts.  # change to scripts folder.\n./create-prerequisites.sh\n```\n\n### **11. Install Kafka ( if installing Business Automation Insights (BAI) )**\n\nFollow the instructions [here](https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_preparing_baik8s_kafka_install.html?_ga=2.8252794.936333667.1604297352-1372577732.1601029870). for different kafka options including IBM Eventstreams.  I have configured IBM Eventstreams v10 using the instructions given in the link above.    \n\n### **12. Update the Custom resource YAML **\n\n\n- See [this](https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/con_capab_ent.html?_ga=2.14983550.936333667.1604297352-1372577732.1601029870) page for understanding patterns and their dependencies. \n- For any pattern, the minimum required configuration parameters can be found in \"descriptors/patterns/ibm_cp4a_cr_enterprise_< capability >.yaml\"  and\n- For any pattern,  fully customizable list of  parameters can be found in  \"descriptors/patterns/ibm_cp4a_cr_enterprise_FC_< capability >.yaml\"\n- If only specific capabilities are being installed, the required parameters for that capability can be identified by looking at the capability specific yaml file described described in 2 and 3.\n- If installing multiple capabilities, the parameters need to be combined to one yaml file and the sample file referred below is an example of that.\n- Create a copy of descriptors/patterns/ibm_cp4a_cr_enterprise_sample.yaml and make the below updates in the copy.\n- Remove configuration for applications which are not of interest. ( for e.g. ca_configuration is for ACA, bastudio_configuration is for BAStudio and so on. )\n- Update fields below for different components ( by replacing values in < > with actual values)\n- Make sure datasource_configuration is uncommented for the component being installed.\n\n \n\n1.Shared Configuration:\n\n    shared_configuration.sc_deployment_patterns:  <as per the components being installed >\n    shared_configuration.sc_optional_components: <as per the components being installed >\n    shared_configuration.sc_deployment_hostname_suffix: \"cp-automation.<registered-domain or <public ip of loadbalancer>.nip.io>\"\n    shared_configuration.kafka_configuration.bootstrap_servers:  <eventstreams bootstrap server route>:443\n\n2.LDAP Configuration\n\n    ldap_configuration.lc_ldap_server : <server configured in step 5>\n    ldap_configuration.lc_ldap_group_base_dn : <if base DN was modified during LDAP setup>\n\n3.UMS Configuration\n\n    datasource_configuration.dc_ums_datasource.dc_ums_oauth_host: <server configured in step 5>\n    datasource_configuration.dc_ums_teamserver_host.dc_ums_oauth_host: <server configured in step 5>\n\n        \n4.CA Configuration\n\n    datasource_configuration.dc_ca_datasource.database_servername: <server configured in step 5\n\n5.BAStudio\n\n    bastudio_configuration.database.host: <server configured in step 5>\n    playback_server.database.host: <server configured in step 5\n\n6.ODM\n\n    datasource_configuration.dc_odm_datasource.database_servername: <server configured in step 5>\n\n7.Filenet\n\n    datasource_configuration.dc_gcd_datasource.database_servername: <server configured in step 5>\n    datasource_configuration.dc_os_datasources.database_servername: <server configured in step 5>\n    #verify_configuration and initialize_configuration sections are configured with default userids, tablespace and database names\n    #If you are using different from default then update accordingly.\n\n8.BAN ( ICN)\n\n    datasource_configuration.dc_icn_datasource.database_servername: <server configured in step 5>\n\n9.BAW \n\n    baw_configuration.hostname: baw1.<hostname_suffix>\n    baw_configuration.database.server_name : <db server ip>\n    baw_configuration.database.database_name: <db name>\n    pfs_configuration.hostname:pfs.<hostname_suffix>\n    datasource_configuration.dc_os_datasources.database_servername: <db server for  DOS db>\n    datasource_configuration.dc_os_datasources.database_servername: <db server for TOS db>\n\n10.BAI ( has some isssues in v20.0.2 on ROKS )\n\n### **13. Install components**\n\n    ```\n    oc apply -f descriptors/ibm_cp4a_cr_enterprise_sample.yaml\n    ```\n\n**Check Operator Logs:**\n\n    oc logs -f operator-pod -c operator\n\n**check Ansible Logs** ( Updated after one round of operator reconcilliation for all components is complete ..varies from 8 – 25 mins ) :\n\n    oc logs -f operator-pod -c ansible\n   \n### **14. Post install steps **\n\nCheck out the [Knowledge Center](https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_deploy_postdeployk8s.html?_ga=2.214745823.936333667.1604297352-1372577732.1601029870) for post install steps for different components\n\nUse the command “oc get routes” to get routes for all the installed components. Given below are some of the component URLs.\n\n \n\n1.**UMS Login:**\n\n    https://ums.< HOSTNAME_SUFFIX >/ums/ umsadmin/password\n\n2.**BACA:**\n\n- See the steps documented here. for details. High level steps outlined below.\n- First hit the backend url ( required for every browser for the first time) ( not required if using routes secured by well known CA) : https://backend.<HOSTNAME_SUFFIX>\n- Hit frontend URL : https://frontend.< HOSTNAME_SUFFIX >?tid=t4900&ont=default ( t4900 = tenant ID set during CATenant DB creation. and default = ontology name set during CATenant DB creation).\n\n3.**BAStuido Login:**\n\n    https://bas.<HOSTNAME_SUFFIX>/BAStudio umsadmin/password\n\n4.**ODM Login:**\n\n- https://decisionserverconsole.odm.icp4adeploy.< HOSTNAME_SUFFIX >\n- https://decisioncenter.odm.icp4adeploy.<HOSTNAME_SUFFIX>\n- For userId check file ODM/configuration/security/sample-webSecurity-OIDC-ums.xml. ( default is orgAdmin)\n\n5.**BAI: **\n\nRead throug the steps [here](https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_post_bai_deploy.html?_ga=2.217819230.936333667.1604297352-1372577732.1601029870). for understanding how to make Kibana and Elastic search accessible outside the cluster.\n\n6.**FNCM:**\n\n- https://cpe-automation.<HOSTNAME_SUFFIX>/P8CE/Health ceadmin/passw0rd\n- Check that there are no X symbols, which would mean something couldn’t be created.\n\n7.**Navigator:*\n\n- https://navigator.<HOSTNAME_SUFFIX>/navigator ceadmin/passw0rd\n- Check that you can see the “ICN desktop” in the list of desktops ( created by initialization script)\n- https://navigator.<HOSTNAME_SUFFIX>/navigator?desktop=demo\n\n### ** Troubleshooting  **\n\n\n**ODM Issues** \n\nicp4adeploy-default-deny network policy prevents egress from ODM pods to UMS pods so add a policy manually to allow that.\n\n```\noc edit networkpolicy icp4adeploy-odm-dc-network-policy -o yaml\n```\nUpdate the spec portion of  yaml with following content to allow egress\n\n```\n    spec:\n      egress:\n      – {}\n      ingress:\n        ports:\n        – port: 9453\n          protocol: TCP\n      podSelector:\n        matchLabels:\n          run: icp4adeploy-odm-decisioncenter\n      policyTypes:\n      – Egress\n      – Ingress\n```\n\nIn case there is an error about unknown Client_ID when the browser redirects to UMS,  the CLIENT_ID registration wasn’t successful, which Can be confirmed by looking at the icp4adeploy-odm-oidc-job-registration-* pod logs. \n\nIn this case, manually register the ODM client to UMS.\n\nDownload the postman file here, replace < HOSTNAME_SUFFIX > with the value for your environment.  Confirm that the redirect_uris match with the URIs shown in the icp4adeploy-odm-oidc-job-registration-* pod logs.\n\n** Troubleshooting BAI - TBD**\n\n**If operator pod hangs**\n\nsometimes, the operator logs show no update for a long time e.g. 20 mins. In that case one can delete the operator pod and see if that fixes the issue\n\n \n\noc delete < operator pod > -n automation.\n","type":"Mdx","contentDigest":"b384f7741d2771fbd8dff5347641f354","counter":940,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"CP4 Automation"},"exports":{},"rawBody":"---\ntitle: CP4 Automation\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\n</PageDescription>\n\n## **Automation overview**\n\nDigital business automation (DBA) allows an organization to improve its operations by streamlining the way people participate in business processes and workflows, automate repeatable decisions, and provide business users with the ability to edit and change the business logic involved in these business processes. DBA projects also aim to make documents easy to store and retrieve, digitize document content, such as with optical character recognition (OCR), and automate data entries with software robots, also referred to as robotic process automation.\n\nThe IBM Cloud Pak for Automation offers a software platform to develop, deploy, run and manage your digital business automation projects, using the capabilities shown in the following digram: \n\n## **Receipe Overview**\n\nThe intent of this recipe is to provide a simplified set to steps, to set up an instance for PoC or Demos with least effort. Viewer is advised to refer to the original Enterprise install steps once before going ahead with these steps.\nIngredients\n\n\n1. IBM ROKS Openshift cluster v4.3  (  6 worker nodes with 8 cores x 16 GB RAM )\n\n2. A RHEL 7.x VM ( 4 cores x 16 GB RAM) for  DB2, IBM security Directory server and NFS file share.\n\n3. Cloud Pak for Automation v20.0.1\n\n4. DB2 v11.1\n\n5. IBM Security Directory Server v6.4\n\n6. workstation with oc, kubectl, git  installed for executing the steps during install.\n\n## **Step-by-step**\n\n### **1. Provision Openshift Cluster**\n\nProvision an Openshift cluster v4.3 with 6 worker nodes having 8 cores x 16 GB RAM , on IBM Cloud. If it’s provisioned by someone else, the person following the instructions, should have Admin access to the cluster.  \n\n### **2.Provision 1 or more VMs for shared services. ( 1 would suffice)**\n\nSome of the install instructions related to DB2 and TDS (Tivoli Directory Server , which is now called – IBM Security Directory server )   require use of RPM so one of the supported linux distros is recommended. Steps have been tested with RHEL 7.x.  ( 4 cores x 16 GB RAM) . 4 x 8 should also suffice.\n\n### **3. Clone the github repository for helper scripts**\n\n1.The original (official)  documentation can be found in Knowledge Center. It refers to github repo:  https://github.com/icp4a/cert-kubernetes/tree/20.0.2 for helper scripts.\n\n2. Some additional helper scripts (with some assumptions for Enterprise installs for dev environments ) have been added in this fork of the official project here:  https://github.com/sachinjha/cert-kubernetes/tree/20.0.2\n\n3. Going through the knowledge Center link above is recommended to understand the overall install plan. In knowledge Center, refer to “Enterprise Install” options and anything that applies to “ROKS” ( Redhat Openshift Kubernetes Service , on IBM Cloud ) , as that is our environment.  Instructions which apply to “OCP” but are not supported on ROKS are not applicable. Look for correspoding options for ROKS.\n     \n### **4.Setup NFS services**\n1. The storage capacity in default disk attached to VMs may not be sufficient. (Default is 25GB and max is 100GB). It’s recommended to add an additional disk (File Storage – 750 GB or more)  from IBM cloud console and use that for NFS.\n        \n2. Once you have added disk run – ‘fdisk -l’ to view the disks attached. See image below for example. here /dev/xvdc is the additional device.\n\n3. ![IBM Cloud Automation NFS](images/Automation-NFs.png)\n\n4. Run folowing commands to create filesystem and mount the disk.\n    - mkfs.ext4 /dev/xvdc\n    - mkdir /opt/nfs2\n    - mount /dev/xvdc /opt/nfs2\n        \n5. Follow the steps [here](https://linuxconfig.org/quick-nfs-server-configuration-on-redhat-7-linux) for setting up NFS service on RHEL\n\n6. Perform following steps to configure the folders required for ICP4Auto components and for updating /etc/exports for NFS\n    1. copy files ( configure-nfs.sh  and nfs-exports-configuration.txt)  under this link , in the git project to NFS server.\n    \n    2. configure-nfs.sh will create all the folders with required permissions for components which need NFS based storage.\n    \n    3. nfs-exports-configuration.txt  has the place holders for /etc/exports.\n    \n    4. Replace the W*Public and W*Private  in nfs-exports-configuration.tx with Public and Private IPs of 4 worker nodes. \n   \n    5. chmod +x configure-nfs.sh\n   \n    6. ./configure-nfs.sh\n   \n    7. Verify that  nfs status is green and  /etc/exports is updated with configuration\n \n### **5. Setup other shared Services - LDAP and DB2**\n\n\n####  LDAP and DB2 installation:\n1. Follow the steps on this page for installing DB2 and LDAP.  Instructions are for Linux only. Read the complete instructions for the section before proceeding to perform the steps. Some troubleshooting steps for DB2 install can be found [here](https://github.com/sachinjha/cert-kubernetes/tree/20.0.2/shared-services/db2.md).    \n2. Run the following commands after Create Instance Step in SDS setup, before starting the server. In case you have started it, you can stop it, using the  commands given in the link, peform the steps below and then start it again.\n\n\n\n```sh\n# Configure a database for a directory server instance.\n./idscfgdb -I dsinst1 -a dsinst1 -w <your-password> -t dsinst1 -l /home/dsinst1\n\n# Set the administration DN and administrative password for an instance\n./idsdnpw -I dsinst1 –u cn=root –p <your-password>\n\n# Add suffix\n./idscfgsuf -I dsinst1 -s o=IBM,c=US\n```\n\nUse the LDIF file [referenced here](https://cloudpak8s.io/automation/shared-services/#import-ldap-users-and-groups) to add some default users to LDAP.\n\nNote: One can change the suffix to suit their org and country for e.g. o=ABC,c=IN but make sure to update the references in LDIF file given in the above link,  before importing it into LDAP and also in the CR yaml (icp4acluster) used for installing the components.\n\n \n\n#### 2nd instance of LDAP ( TDS )\n2nd instance of LDAP ( TDS ) is  required for BAN and FNCM if you want external users to access. Steps to create 2nd instance are [here](https://github.com/sachinjha/cert-kubernetes/blob/20.0.1/shared-services/tds%20ext%20ldap%20creation.sh).\n\n\n#### Security Groups for LDAP and DB server \nMake sure the security group attached to the VM allows inbound on port:  22(ssh),  50000 (db2) , 389 (ldap) , 390 ( ext ldap) , 2049( nfs)   and outbound on ports :  53  (dns) , 443 ( https) , 80 ( http) )\n\n### **6. Identify the suffix for Openshift routes and secure routes**\n\n**Use the following option if routes with self signed certificates are not a problem.**\n\n1. Get public IP of loadbalancer using any of the methods given in the image below.\n    \n2. ![IBM Cloud openshift routes](images/Automation-Routes.png)\n\n3. Use a suffix like  publicIP.nip.io\n\n4. For example, in this case ,  suffix can be “169.44.184.38.nip.io”  and a sample hostname for UMS service, can be “ums.169.44.184.38.nip.io”\n     \n**In case there is a need to create secure routes with certificates signed by known CAs then follow the steps below: ( This hasn’t been completely tested for all components )**\n        \n1. ***Update on 31st Aug 2020- secure routes not getting created as expected, with Let’s Encrypt intermediate_cert, so one may choose the above option, or stop at 7 and use the registered domain, allowing operator to generate the certificates.***\n2. In IBM Cloud ->Classic Infrastruction -> Services -> Domain Registration, register a domain  ( e.g. automation.org.xyz.com )  \n3. Create an instance of  \"IBM Internet services\", on IBM cloud  and update the custom name servers in “Domain Registration”  to the name servers given in “IBM Internet Services” setup page.\n4. ![IBM Cloud Automation NFS](images/internet-services.png)\n5. ![IBM Cloud Automation NFS](images/domain-reg.png)\n\n6. Updation of records might take 24 hrs to reflect in \"internet services\"\n7. In \"internet services\" Create a wildcard subdomain  A record  * which maps to the above loadbalancer IP. e.g. *.cp-automation -> < public IP found above >\n8. Create an instance of \"certificate manager\" and select wild card subdomain as CN for certificates.\n9. For Example : Generate an SSL certificate with wild card subdomain ( *.cp-automation.org.xyz.com ) \n10.Let's encrypt certificate should give you key, cert and Intermediate Cert ( which is authorized to issue certs, and can be used as CA) . We will use this to create a secret which will act as the root-ca-secret and will be referred in the Cloud Pak for Automation Custom resource file\n\n### **7. Next steps - Follow Knowledge Center**\n\nSimplified steps are given below i.e. Step 8 onwards  but its important to understand the process as documented in the  Knowledge Center .\n\n1. On ROKS, you need to install cluster manually. \n\n3. Look at Preparing to install Containers for components of interest.\n\n    Foundation pattern is a prerequisite for any other pattern, so start with that and then add different patterns to the custom resource file as required.\n\n4. Deploy custom resource after manually updating the sections for different components with values that need to be configured.\n\n### **8. Create DB2 databases required for different components**\n\nSteps for components which are not of interest can be skipped. UMS should be done as UMS is required for all other components.\n\nExecute all the scripts as user db2inst1. On the DB server, ensure that db2 is in PATH  by running command below and then follow the instructions below for each of the components:\n```     \nsu db2inst1\n./home/db2inst1/sqllib/db2profile\n```\n     \n1. **UMS**\n    - copy [file](https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/UMS/configuration/create-db.sh) or contents of the file and execute them on DB server.\n    \n2. **BACA**\n    - copy [DB2](https://github.com/sachinjha/cert-kubernetes/tree/20.0.2/ACA/configuration-ha/DB2) folder to DB server.\n    - Execute the scripts CreateBaseDB.sh  and AddTenant.sh.\n    - See [log](https://github.com/sachinjha/cert-kubernetes/tree/20.0.1/ACA/configuration-ha/db-creation-logs) files for sample values that can be used. ( mostly defaults )\n        ( bacaadmin is a user created in LDAP when the LDIF file is imported. If you change users, use it accordingly when executing the script)\n\n3. **ODM**\n    - copy [file](https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/ODM/configuration/create-db.sh) or contents of the file and execute them on DB server.\n     \n4. **ICN**\n    \n    - copy [file](https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/BAN/configuration/create-db.sql)   or contents of the file and execute them on DB server\n\n    ```\n    db2 -tvf create-db.sql\n    ```\n\n5. **FNCM**\n    - copy [DB2](https://github.com/sachinjha/cert-kubernetes/tree/20.0.2/FNCM/configuration/DB2) folder to DB server.\n    - Execute the scripts\n        \n    ```\n    ./create-gcd.sh \n    ./create-os.sh\n    ```\n\n6. **APPENG**\n\n    - copy [file](https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/BAS/configuration/appenng-db2.sql) or contents of the file and execute them on DB server\n\n    ```\n    db2 -tvf appeng-db2.sql\n    ```        \n    \n7. **BAStudio**\n\n    - copy [db2](https://github.com/sachinjha/cert-kubernetes/tree/20.0.2/BAS/configuration/db2) folder to DB server.\n    - Use db2 command to execute the .sql files\n\n    ```\n    db2 -tvmf bastudio.sql\n    db2 -tvmf appengine.sql\n    ```\n\n8.  **BAW ( on containers)**\n        \n    - copy file to DB server.\n    - Use db2 command to execute the .sql files\n            \n            \n    ```\n    db2 -tvmf  create-db.sql\n    ```\n\n    - Use the create-os.sh script from FNCM (in step 5) and create 2 Object storage dbs  DOSDB and TOSDB.  Just the db names are different, rest of the values in the script file should be same.\n\n      \n\n\n### **9.Install Operator**\n\n1.\n    ```\n    oc new-project automation.\n    ```\n2. If you use a different name for the project, you may have to edit multiple files in the next step, where namespaces may be hard coded.\n\n3. On ROKS, you need to install cluster manually.  Before creating the operator, look at the\n\n    a) [link for preparing Storage](https://www.ibm.com/support/knowledgecenter/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_prep_operator.html?_ga=2.20303472.936333667.1604297352-1372577732.1601029870) for Operator.  Create storage classes using the sample files given in the project and use one of them for operator-shared-pvc. For e.g. cp4a-file-retain-bronze-gid. Ignore step 6 and 7 at this stage.\n\n    b)  [Getting access to container images](https://developer.ibm.com/recipes/tutorials/access%20to%20container%20images): You can use entitled registry. When creating the secret set docker-username=cp\n\n4. Now create the common service and Operator using the steps given here. If you are going to configure BAI and going to configure IBM Eventstreams for that then execute the create common services script with argument “bai” e.g.\n    \n    ```\n    ./deploy_CS3.4.sh bai\n    ```\n\n5. Now [Install operator](https://www.ibm.com/support/knowledgecenter/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_setup_cluster_manually.html?_ga=2.218958161.936333667.1604297352-1372577732.1601029870)\n6. Copy jdbc jars  to operator pod by following steps [6 and 7 here](https://www.ibm.com/support/knowledgecenter/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_prep_operator.html?_ga=2.20303472.936333667.1604297352-1372577732.1601029870).\n\n\n\n\n\n###  **10.Create secrets , Rolebindings, PVs and PVCs for different components. **\n\nAfter updating the component related files, do not forget to tun the script “create_preequisites.sh” to apply the changes .\n\nPWD = Root folder of cert_kubernetes project.\n\n1.UMS\n    - Update the value of keys – oauthDBPassword and tsDBPassword in UMS/configuration/secrets.yaml with password for db2inst1 set during step 5.\n    - Update the values of keys externalLdapPassword,  ldapPassword and lc_ldap_bind_password to values set during LDAP installation in step 5 in UMS/configuration/ldap-bind-secret.sh\n2.BACA\n    - Update the (file)[https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/ACA/configuration-ha/security/aca-basedb-secret.sh] with db2 password configured in step 5.\n3.ODM\n    - Update the (file)[https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/ODM/configuration/create-db.sh] with db2 password configured in step 5.\n    - If configuring integration with UMS, update (file)[https://github.com/sachinjha/cert-kubernetes/blob/20.0.2/ODM/configuration/security/sample-websecurity-OIDC-ums.xml] with different username or ldap group and Suffix if the default values are not suitable.\n4.BAS \n    - ( Includes Resource Registry and App Engine) Update BAS/configuration/db2/secret.yaml by replacing values of “dbPassword” and “AE_DATABASE_PWD” with password set for db2inst1 in Step 5.\n5.BAI\n    - Update BAI/configuration/bai-psp.yaml and replace CR_NAME with actual custom resource name.\n    - update create-prerequisites.sh and modify icp4adeploy-bai-psp-sa  in “BAI” section according to CR_NAME if it’s different from icp4adeploy.\n6.BAN\n    - Update the values of navigatorDBPassword, ldapPassword , externalLdapPassword in BAN/configuration/ban-secret.sh\n7.FNCM\n    - Update the values of gcdDBPassword, osDBPassword , ldapPassword in FNCM/configuration/ibm-fncm-secret.sh\n    - Update the value of <NFS SERVER> in FNCM/configuration/volumes_cmis.yaml,  graphql_volume.yaml and volumes_cpe.yaml  with with NFS server IP configured in step 4\n8.App Engine:\n    - Update the values of AE_DATABASE_PWD in BAS/configuration/secret.yaml with DB2 password\n9.BAW on containers: \n    - Update the values of db2 passowrd in the file : BAW/configuration/encryption-key-secret.yaml\n\n \n\n ```\ncd scripts.  # change to scripts folder.\n./create-prerequisites.sh\n```\n\n### **11. Install Kafka ( if installing Business Automation Insights (BAI) )**\n\nFollow the instructions [here](https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_preparing_baik8s_kafka_install.html?_ga=2.8252794.936333667.1604297352-1372577732.1601029870). for different kafka options including IBM Eventstreams.  I have configured IBM Eventstreams v10 using the instructions given in the link above.    \n\n### **12. Update the Custom resource YAML **\n\n\n- See [this](https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/con_capab_ent.html?_ga=2.14983550.936333667.1604297352-1372577732.1601029870) page for understanding patterns and their dependencies. \n- For any pattern, the minimum required configuration parameters can be found in \"descriptors/patterns/ibm_cp4a_cr_enterprise_< capability >.yaml\"  and\n- For any pattern,  fully customizable list of  parameters can be found in  \"descriptors/patterns/ibm_cp4a_cr_enterprise_FC_< capability >.yaml\"\n- If only specific capabilities are being installed, the required parameters for that capability can be identified by looking at the capability specific yaml file described described in 2 and 3.\n- If installing multiple capabilities, the parameters need to be combined to one yaml file and the sample file referred below is an example of that.\n- Create a copy of descriptors/patterns/ibm_cp4a_cr_enterprise_sample.yaml and make the below updates in the copy.\n- Remove configuration for applications which are not of interest. ( for e.g. ca_configuration is for ACA, bastudio_configuration is for BAStudio and so on. )\n- Update fields below for different components ( by replacing values in < > with actual values)\n- Make sure datasource_configuration is uncommented for the component being installed.\n\n \n\n1.Shared Configuration:\n\n    shared_configuration.sc_deployment_patterns:  <as per the components being installed >\n    shared_configuration.sc_optional_components: <as per the components being installed >\n    shared_configuration.sc_deployment_hostname_suffix: \"cp-automation.<registered-domain or <public ip of loadbalancer>.nip.io>\"\n    shared_configuration.kafka_configuration.bootstrap_servers:  <eventstreams bootstrap server route>:443\n\n2.LDAP Configuration\n\n    ldap_configuration.lc_ldap_server : <server configured in step 5>\n    ldap_configuration.lc_ldap_group_base_dn : <if base DN was modified during LDAP setup>\n\n3.UMS Configuration\n\n    datasource_configuration.dc_ums_datasource.dc_ums_oauth_host: <server configured in step 5>\n    datasource_configuration.dc_ums_teamserver_host.dc_ums_oauth_host: <server configured in step 5>\n\n        \n4.CA Configuration\n\n    datasource_configuration.dc_ca_datasource.database_servername: <server configured in step 5\n\n5.BAStudio\n\n    bastudio_configuration.database.host: <server configured in step 5>\n    playback_server.database.host: <server configured in step 5\n\n6.ODM\n\n    datasource_configuration.dc_odm_datasource.database_servername: <server configured in step 5>\n\n7.Filenet\n\n    datasource_configuration.dc_gcd_datasource.database_servername: <server configured in step 5>\n    datasource_configuration.dc_os_datasources.database_servername: <server configured in step 5>\n    #verify_configuration and initialize_configuration sections are configured with default userids, tablespace and database names\n    #If you are using different from default then update accordingly.\n\n8.BAN ( ICN)\n\n    datasource_configuration.dc_icn_datasource.database_servername: <server configured in step 5>\n\n9.BAW \n\n    baw_configuration.hostname: baw1.<hostname_suffix>\n    baw_configuration.database.server_name : <db server ip>\n    baw_configuration.database.database_name: <db name>\n    pfs_configuration.hostname:pfs.<hostname_suffix>\n    datasource_configuration.dc_os_datasources.database_servername: <db server for  DOS db>\n    datasource_configuration.dc_os_datasources.database_servername: <db server for TOS db>\n\n10.BAI ( has some isssues in v20.0.2 on ROKS )\n\n### **13. Install components**\n\n    ```\n    oc apply -f descriptors/ibm_cp4a_cr_enterprise_sample.yaml\n    ```\n\n**Check Operator Logs:**\n\n    oc logs -f operator-pod -c operator\n\n**check Ansible Logs** ( Updated after one round of operator reconcilliation for all components is complete ..varies from 8 – 25 mins ) :\n\n    oc logs -f operator-pod -c ansible\n   \n### **14. Post install steps **\n\nCheck out the [Knowledge Center](https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_deploy_postdeployk8s.html?_ga=2.214745823.936333667.1604297352-1372577732.1601029870) for post install steps for different components\n\nUse the command “oc get routes” to get routes for all the installed components. Given below are some of the component URLs.\n\n \n\n1.**UMS Login:**\n\n    https://ums.< HOSTNAME_SUFFIX >/ums/ umsadmin/password\n\n2.**BACA:**\n\n- See the steps documented here. for details. High level steps outlined below.\n- First hit the backend url ( required for every browser for the first time) ( not required if using routes secured by well known CA) : https://backend.<HOSTNAME_SUFFIX>\n- Hit frontend URL : https://frontend.< HOSTNAME_SUFFIX >?tid=t4900&ont=default ( t4900 = tenant ID set during CATenant DB creation. and default = ontology name set during CATenant DB creation).\n\n3.**BAStuido Login:**\n\n    https://bas.<HOSTNAME_SUFFIX>/BAStudio umsadmin/password\n\n4.**ODM Login:**\n\n- https://decisionserverconsole.odm.icp4adeploy.< HOSTNAME_SUFFIX >\n- https://decisioncenter.odm.icp4adeploy.<HOSTNAME_SUFFIX>\n- For userId check file ODM/configuration/security/sample-webSecurity-OIDC-ums.xml. ( default is orgAdmin)\n\n5.**BAI: **\n\nRead throug the steps [here](https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_20.0.x/com.ibm.dba.install/op_topics/tsk_post_bai_deploy.html?_ga=2.217819230.936333667.1604297352-1372577732.1601029870). for understanding how to make Kibana and Elastic search accessible outside the cluster.\n\n6.**FNCM:**\n\n- https://cpe-automation.<HOSTNAME_SUFFIX>/P8CE/Health ceadmin/passw0rd\n- Check that there are no X symbols, which would mean something couldn’t be created.\n\n7.**Navigator:*\n\n- https://navigator.<HOSTNAME_SUFFIX>/navigator ceadmin/passw0rd\n- Check that you can see the “ICN desktop” in the list of desktops ( created by initialization script)\n- https://navigator.<HOSTNAME_SUFFIX>/navigator?desktop=demo\n\n### ** Troubleshooting  **\n\n\n**ODM Issues** \n\nicp4adeploy-default-deny network policy prevents egress from ODM pods to UMS pods so add a policy manually to allow that.\n\n```\noc edit networkpolicy icp4adeploy-odm-dc-network-policy -o yaml\n```\nUpdate the spec portion of  yaml with following content to allow egress\n\n```\n    spec:\n      egress:\n      – {}\n      ingress:\n        ports:\n        – port: 9453\n          protocol: TCP\n      podSelector:\n        matchLabels:\n          run: icp4adeploy-odm-decisioncenter\n      policyTypes:\n      – Egress\n      – Ingress\n```\n\nIn case there is an error about unknown Client_ID when the browser redirects to UMS,  the CLIENT_ID registration wasn’t successful, which Can be confirmed by looking at the icp4adeploy-odm-oidc-job-registration-* pod logs. \n\nIn this case, manually register the ODM client to UMS.\n\nDownload the postman file here, replace < HOSTNAME_SUFFIX > with the value for your environment.  Confirm that the redirect_uris match with the URIs shown in the icp4adeploy-odm-oidc-job-registration-* pod logs.\n\n** Troubleshooting BAI - TBD**\n\n**If operator pod hangs**\n\nsometimes, the operator logs show no update for a long time e.g. 20 mins. In that case one can delete the operator pod and see if that fixes the issue\n\n \n\noc delete < operator pod > -n automation.\n","fileAbsolutePath":"/home/runner/work/ibm-enterprise-runbooks/ibm-enterprise-runbooks/src/pages/install/automation/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","243517648","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550","817386451"]}