{"componentChunkName":"component---src-pages-install-portworx-for-data-index-mdx","path":"/install/portworx-for-data/","result":{"pageContext":{"frontmatter":{"title":"Portworx setup for Cloud Pak for Data 3.5 on IBM Managed Openshift 4.5+"},"relativePagePath":"/install/portworx-for-data/index.mdx","titleType":"page","MdxNode":{"id":"44fdbf4c-ce76-54bd-b6be-bd5515928004","children":[],"parent":"48c2ac1b-8fca-5990-b4b2-4c46ade7b5e5","internal":{"content":"---\ntitle: Portworx setup for Cloud Pak for Data 3.5 on IBM Managed Openshift 4.5+ \n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\n</PageDescription>\n\n## The following steps provide a fast path to manually complete the installation based on information from [Storing data with Portworx](https://cloud.ibm.com/docs/openshift?topic=openshift-portworx)\n\nEnsure you have access to an OpenShift cluster with the correct size and number of workers. Portworx requires workers that are at least 16cpu/64GB.\n\nFor reference, full documentation on how to provision an OpenShift cluster is [here](https://cloud.ibm.com/docs/openshift?topic=openshift-getting-started#clusters_gs).\n\n## 1. Log in to IBM Cloud CLI\n\nFrom a terminal window, log in to IBM Cloud using the [cli](https://www.ibm.com/cloud/cli) and set the target `resource_group` to the group with your target OpenShift cluster.\n\n    ibmcloud login -sso\n    ibmcloud target -r us-south -g <resource_group>\n    ibmcloud oc clusters\n    ibmcloud oc cluster config -c <cluster_name> –-admin\n\n## 2. Provision Block Storage\n\nFor reference, full documentation on manually provisioning Block Storage is [here](https://cloud.ibm.com/docs/containers?topic=containers-utilities#manual_block).\n\nIssue this command for each of the workers:\n\n    ibmcloud sl block volume-order --storage-type endurance --size <size> --tier 10 --os-type LINUX -d <datacenter>\n\nGet block volume details:\n\n    ibmcloud sl block volume-list\n\nGet worker details:\n\n    ibmcloud oc workers --cluster $CLUSTER\n\nFor each worker, authorize the block storage:\n\n    ibmcloud sl block access-authorize <volume_id> -p <node_private_ip>\n\nFor example:\n\n    ibmcloud sl block access-authorize 214099980 -p 10.185.227.240\n    ibmcloud sl block access-authorize 214100004 -p 10.185.227.238\n    ibmcloud sl block access-authorize 214100042 -p 10.185.227.242\n    ibmcloud sl block access-authorize 214100076 -p 10.185.227.248\n\n## 3. Attach storage to worker nodes\n\n[Classic Infrastructure clusters](#attaching-storage-to-classic-infrastructure-clusters)\n\n[VPC clusters](#attaching-storage-to-vpc-clusters)\n\n### Attaching storage to Classic Infrastructure clusters\n\n#### Install or migrate to Helm version 3 on cluster\n\nFor reference, instructions to install Helm v3 are [here](https://cloud.ibm.com/docs/openshift?topic=openshift-helm#install_v3).\n\n#### Install IBM Cloud Block Storage Attacher Plug-in\n\nFor reference, full documentation on IBM Cloud Block Storage Attacher Plug-in is [here](https://cloud.ibm.com/docs/openshift?topic=openshift-utilities#block_storage_attacher).\n\nIssue these commands:\n\n    oc project kube-system\n    helm repo update\n    helm install block-attacher iks-charts/ibm-block-storage-attacher\n    oc get pod -n kube-system -o wide | grep attacher\n    oc get storageclasses | grep attacher\n\n#### Attach newly created block storage to worker nodes\n\nFor reference, full documentation on how to attach raw block storage to worker nodes is [here](https://cloud.ibm.com/docs/openshift?topic=openshift-utilities#attach_block).\n\nList storage volumes\n\n    ibmcloud sl block volume-list\n\nIssue these two commands for each storage volume to get necessary information to create PVs below:\n\n    ibmcloud sl block access-list <volume_id>\n    ibmcloud sl block volume-detail <volume_id>\n\nCreate a PV to attach each storage block to a worker:\n\n    cat <<EOF | oc create -f -\n    apiVersion: v1\n    kind: PersistentVolume\n    metadata:\n    name: <pv_name>\n    annotations:\n      ibm.io/iqn: \"<IQN_hostname>\"\n      ibm.io/username: \"<username>\"\n      ibm.io/password: \"<password>\"\n      ibm.io/targetip: \"<targetIP>\"\n      ibm.io/lunid: \"<lunID>\"\n      ibm.io/nodeip: \"<private_worker_IP>\"\n      ibm.io/volID: \"<volume_ID>\"\n    spec:\n    capacity:\n      storage: <size>\n    accessModes:\n      - ReadWriteOnce\n    hostPath:\n      path: /\n    storageClassName: ibmc-block-attacher\n    EOF\n\nFor example:\n\n    cat <<EOF | oc create -f -\n    apiVersion: v1\n    kind: PersistentVolume\n    metadata:\n    name: px-pv-240\n    annotations:\n      ibm.io/iqn: \"iqn.2021-02.com.ibm:ibm02su2270065-i168011233\"\n      ibm.io/username: \"IBM02SU2270065-I168011233\"\n      ibm.io/password: \"xxxxxxxxxxxxx\"\n      ibm.io/targetip: \"161.26.110.196\"\n      ibm.io/lunid: \"0\"\n      ibm.io/nodeip: \"10.185.227.240\"\n      ibm.io/volID: \"214099980\"\n    spec:\n    capacity:\n      storage: 600Gi\n    accessModes:\n      - ReadWriteOnce\n    hostPath:\n      path: /\n    storageClassName: ibmc-block-attacher\n    EOF\n\nTo list and describe pv:\n\n    oc get pv\n    oc describe pv <pv_name>\n\nIf attached correctly, you will see `ibm.io/attachstatus: attached`\n\n    Name:            px-pv-238\n    Labels:          <none>\n    Annotations:     ibm.io/attachstatus: attached\n                     ibm.io/dm: /dev/dm-1\n                     ibm.io/iqn: iqn.2021-02.com.ibm:ibm02su2270065-i168011229\n                     ibm.io/lunid: 1\n                     ibm.io/mpath: 3600a09803830566f4724516532536b52\n                     ibm.io/nodeip: 10.185.227.238\n                     ibm.io/password: eRdwGTYSBbCZh444\n                     ibm.io/targetip: 161.26.110.204\n                     ibm.io/username: IBM02SU2270065-I168011229\n                     ibm.io/volID: 214100004\n    Finalizers:      [kubernetes.io/pv-protection]\n    StorageClass:    ibmc-block-attacher\n    Status:          Available\n    Claim:           \n    Reclaim Policy:  Retain\n    Access Modes:    RWO\n    VolumeMode:      Filesystem\n    Capacity:        600Gi\n    Node Affinity:   <none>\n    Message:         \n    Source:\n        Type:          HostPath (bare host directory volume)\n        Path:          /\n        HostPathType:  \n    Events:            <none>\n\n[Proceed to step 4](#4.-install-portworx-on-cluster)\n\n### Attaching storage to VPC clusters\n\nRetrieve IAM token, resource group and cluster.  Set environment variables to save typing in later commands.\n\n    IAM_TOKEN=$(ibmcloud iam oauth-tokens --output json | jq -r '.iam_token')\n    RESOURCE_GROUP=$(ibmcloud target --output json | jq -r '.resource_group.guid')\n    CLUSTER=<clustername>\n\nList storage volumes:\n\n\n    ibmcloud sl block volume-list\n\nGet worker details:\n\n    ibmcloud oc workers --cluster $CLUSTER\n\nUsing the id of the desired worker node and the storage volume, use CLI command to attach:\n\n    ibmcloud ks storage attachment create --cluster $CLUSTER --volume <volume_id> --worker <worker_id>\n\nFor Example:\n\n    ibmcloud ks storage attachment create --cluster $CLUSTER --volume 214099980 --worker kube-c0mktgsd0b2lo0bik8q0-cp4dpartner-default-000001f6\n    ibmcloud ks storage attachment create --cluster $CLUSTER --volume 214100004 --worker kube-c0mktgsd0b2lo0bik8q0-cp4dpartner-default-00000281\n    ibmcloud ks storage attachment create --cluster $CLUSTER --volume 214100042 --worker kube-c0mktgsd0b2lo0bik8q0-cp4dpartner-default-00000314 \n\nVerify attachments:\n\n    curl -X GET \"https://containers.cloud.ibm.com/v2/storage/getAttachments?cluster=$CLUSTER&worker=<worker_ID>\" --header \"X-Auth-Resource-Group-ID: $RESOURCE_GROUP\" --header \"Authorization: $IAM_TOKEN\"\n\n    curl -X GET \"https://containers.cloud.ibm.com/v2/storage/getAttachments?cluster=$CLUSTER&worker=kube-bvc11mbw0n2a9mccenlg-timropwxvpc-default-000002a7\" --header \"X-Auth-Resource-Group-ID: $RESOURCE_GROUP\" --header \"Authorization: $IAM_TOKEN\" {\"volume_attachments\":[{\"id\":\"0757-1e3db03e-e9bb-44ba-b423-14570a3511e6\",\"volume\":{\"name\":\"timro-pwx-vol01\",\"id\":\"r014-aef666d3-5072-4900-a1b2-23a69cb3f96b\"},\"device\":{\"id\":\"0757-1e3db03e-e9bb-44ba-b423-14570a3511e6-8xbd9\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"data\"},{\"id\":\"0757-6a52d8da-3192-4ac9-b78d-3fa3e402be4f\",\"volume\":{\"name\":\"gab-wistful-stimulate-nutmeg\",\"id\":\"r014-54a8d1d0-b679-4c7e-bec6-b91deb30e98f\"},\"device\":{\"id\":\"0757-6a52d8da-3192-4ac9-b78d-3fa3e402be4f-x5rql\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"boot\"}]}\n\n## 4. Install Portworx on cluster\n\n### Create an API Key\n\nFor reference, full documentation on how to create an API key is [here](https://cloud.ibm.com/docs/account?topic=account-userapikey)\n\nIMPORTANT: Ensure user creating API Key has sufficient permissions to work with cluster. They must be assigned the `Editor` platform role and the `Manager` service access role for `IBM Cloud Kubernetes Service`. For more information, see [User access permissions](https://cloud.ibm.com/docs/containers?topic=containers-access_reference).\n\nNOTE: Portworx won't install using a service id api key.\n\n- Select \"Manage\" -> \"Access (IAM)\" from IBM Cloud Console top menu\n- Select “API Keys” -> \"Create\"\n- Save the value of this key\n\n### Copy pull secrets\n\nCopy from `default` namespace to `kube-system` and associate with service account  (to use KVDB)\n\n    oc get secret all-icr-io -n default -o yaml | sed 's/default/kube-system/g' | oc create -n kube-system -f -\n    oc patch -n kube-system serviceaccount/default --type='json' -p='[{\"op\":\"add\",\"path\":\"/imagePullSecrets/-\",\"value\":{\"name\":\"all-icr-io\"}}]'\n    oc describe serviceaccount default -n kube-system\n\n### Use Public Catalog to install Portworx Enterprise\n\nFor reference, full documentation on how to install Portworx Enterprise is [here](https://cloud.ibm.com/docs/openshift?topic=openshift-portworx#install_portworx)\n\nFor non-production scenario, skip encryption. Choose to use in-cluster KVDB for simplicity.\n\nOpen the [Portworx catalog tile](https://cloud.ibm.com/catalog/services/portworx-enterprise):\n\n- Select Datacenter\n- Choose Pricing plan. For non production scenario, Disaster Recovery isn't needed\n- Enter a service name\n- Select resource group\n- Provide API key created in step 7 which will produce list of available clusters\n- Choose the target cluster\n- Under `tags` provide the cluster name. Do not provide any other tags.\n- Enter a Portworx cluster name\n- Select \"portworx KVDB\" in the pulldown for metadata key-value store\n- Select \"Kubernetes secret\" as the secret_type\n\n![portworx options](./images/portworx_options.png)\n\nAfter the service shows as active in the IBM Cloud resource view, verify the deployment:\n\n    kubectl get pods -n kube-system | grep 'portworx\\|stork'\n\nThis should display something like the following:\n\n    portworx-647c5                            1/1     Running     0          9m33s\n    portworx-api-h7dnr                        1/1     Running     0          9m33s\n    portworx-api-ndpxb                        1/1     Running     0          9m33s\n    portworx-api-srnjk                        1/1     Running     0          9m33s\n    portworx-gzgqc                            1/1     Running     0          9m33s\n    portworx-pvc-controller-b8c88b4d7-6rnq6   1/1     Running     0          9m33s\n    portworx-pvc-controller-b8c88b4d7-9bfxk   1/1     Running     0          9m33s\n    portworx-pvc-controller-b8c88b4d7-nqqpr   1/1     Running     0          9m33s\n    portworx-vxphk                            1/1     Running     0          9m33s\n    stork-6f74dcf5fc-mxwxb                    1/1     Running     0          9m33s\n    stork-6f74dcf5fc-svnrl                    1/1     Running     0          9m33s\n    stork-6f74dcf5fc-z9qlc                    1/1     Running     0          9m33s\n    stork-scheduler-7d755b5475-grzr2          1/1     Running     0          9m33s\n    stork-scheduler-7d755b5475-nl25m          1/1     Running     0          9m33s\n    stork-scheduler-7d755b5475-trhhb          1/1     Running     0          9m33s\n\nUsing one of the portworx pods, check the status of the storage cluster\n\n    kubectl exec portworx-647c5 -it -n kube-system -- /opt/pwx/bin/pxctl status\n\nThis should produce output like:\n\n    Status: PX is operational\n    License: PX-Enterprise IBM Cloud (expires in 1201 days)\n    Node ID: 5d65ce5b-1333-4b0c-b469-ccf7df1ce94a\n      IP: 172.26.0.10 \n      Local Storage Pool: 1 pool\n      POOL    IO_PRIORITY  RAID_LEVEL  USABLE    USED     STATUS  ZONE      REGION\n      0       LOW          raid0       400 GiB   18 GiB   Online  us-east-1 us-east\n      Local Storage Devices: 1 device\n      Device  Path      Media Type               Size     Last-Scan\n      0:1     /dev/vdd  STORAGE_MEDIUM_MAGNETIC  400 GiB  18 Dec 20 04:43 UTC\n      * Internal kvdb on this node is sharing this storage device /dev/vdd  to store its data.\n      total   -         400 GiB\n      Cache Devices:\n        * No cache devices\n    Cluster Summary\n      Cluster ID: pwx-iaf\n      Cluster UUID: 45fc03a8-7e82-497d-bc2a-0844dca1459f\n      Scheduler: kubernetes\n      Nodes: 3 node(s) with storage (3 online)\n      IP           ID                                   SchedulerNodeName  StorageNode  Used    Capacity  Status  StorageStatus Version         Kernel                      OS\n      172.26.0.9   f96e278c-fd06-42a8-9684-0d91bc0bde9c 172.26.0.9         Yes          18 GiB  400 GiB   Online  Up            2.6.1.6-3409af2 3.10.0-1160.6.1.el7.x86_64  Red Hat\n      172.26.0.10  5d65ce5b-1333-4b0c-b469-ccf7df1ce94a 172.26.0.10        Yes          18 GiB  400 GiB   Online  Up (This node 2.6.1.6-3409af2 3.10.0-1160.6.1.el7.x86_64  Red Hat\n      172.26.0.11  1b56ec6c-6dcd-4807-a9cd-cf1ae12e7635 172.26.0.11        Yes          18 GiB  400 GiB   Online  Up            2.6.1.6-3409af2 3.10.0-1160.6.1.el7.x86_64  Red Hat\n      Warnings: \n          WARNING: Internal Kvdb is not using dedicated drive on nodes [172.26.0.11 172.26.0.9 172.26.0.10]. This configuration is not recommended for production clusters.\n    Global Storage Pool\n      Total Used      :  53 GiB\n      Total Capacity  :  1.2 TiB\n\nTo review classificiation:\n\n    kubectl exec -it <portworx_pod> -n kube-system -- /opt/pwx/bin/pxctl cluster provision-status\n\nTest by creating a PVC - create a yaml file with a read-write-many claim:\n\n    cat <<EOF | oc create -f -\n    kind: PersistentVolumeClaim\n    apiVersion: v1\n    metadata:\n      name: mypvc\n    spec:\n      accessModes:\n      - ReadWriteMany\n      resources:\n        requests:\n          storage: 10G\n      storageClassName: portworx-shared-sc\n    EOF\n\n## 5. Create Portworx storage class definitions for Cloud Pak for Data\n\nCloud Pak for Data requires certain storage classes. To define them manually, go [here](https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_latest/cpd/install/portworx-storage-classes.html) to see the definitions.\n\nFor example `portworx-shared-gp3` is defined by:\n\n    cat <<EOF | oc create -f -\n    apiVersion: storage.k8s.io/v1\n    kind: StorageClass\n    metadata:\n      name: portworx-shared-gp3\n    parameters:\n      priority_io: high\n      repl: \"3\"\n      sharedv4: \"true\"\n      io_profile: db_remote\n      disable_io_profile_protection: \"1\"\n    allowVolumeExpansion: true\n    provisioner: kubernetes.io/portworx-volume\n    reclaimPolicy: Retain\n    volumeBindingMode: Immediate\n    EOF\n\n## Cleaning up and removing Portworx\n\nRun in the cluster:\n\n    curl -fsL https://install.portworx.com/px-wipe | bash\n\nRemove the resource from the IBM Cloud Console with delete, then deprovision the OpenShift Cluster\n","type":"Mdx","contentDigest":"0e73f045bdf96c7cb3992ca1ff99993c","counter":824,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Portworx setup for Cloud Pak for Data 3.5 on IBM Managed Openshift 4.5+"},"exports":{},"rawBody":"---\ntitle: Portworx setup for Cloud Pak for Data 3.5 on IBM Managed Openshift 4.5+ \n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\n</PageDescription>\n\n## The following steps provide a fast path to manually complete the installation based on information from [Storing data with Portworx](https://cloud.ibm.com/docs/openshift?topic=openshift-portworx)\n\nEnsure you have access to an OpenShift cluster with the correct size and number of workers. Portworx requires workers that are at least 16cpu/64GB.\n\nFor reference, full documentation on how to provision an OpenShift cluster is [here](https://cloud.ibm.com/docs/openshift?topic=openshift-getting-started#clusters_gs).\n\n## 1. Log in to IBM Cloud CLI\n\nFrom a terminal window, log in to IBM Cloud using the [cli](https://www.ibm.com/cloud/cli) and set the target `resource_group` to the group with your target OpenShift cluster.\n\n    ibmcloud login -sso\n    ibmcloud target -r us-south -g <resource_group>\n    ibmcloud oc clusters\n    ibmcloud oc cluster config -c <cluster_name> –-admin\n\n## 2. Provision Block Storage\n\nFor reference, full documentation on manually provisioning Block Storage is [here](https://cloud.ibm.com/docs/containers?topic=containers-utilities#manual_block).\n\nIssue this command for each of the workers:\n\n    ibmcloud sl block volume-order --storage-type endurance --size <size> --tier 10 --os-type LINUX -d <datacenter>\n\nGet block volume details:\n\n    ibmcloud sl block volume-list\n\nGet worker details:\n\n    ibmcloud oc workers --cluster $CLUSTER\n\nFor each worker, authorize the block storage:\n\n    ibmcloud sl block access-authorize <volume_id> -p <node_private_ip>\n\nFor example:\n\n    ibmcloud sl block access-authorize 214099980 -p 10.185.227.240\n    ibmcloud sl block access-authorize 214100004 -p 10.185.227.238\n    ibmcloud sl block access-authorize 214100042 -p 10.185.227.242\n    ibmcloud sl block access-authorize 214100076 -p 10.185.227.248\n\n## 3. Attach storage to worker nodes\n\n[Classic Infrastructure clusters](#attaching-storage-to-classic-infrastructure-clusters)\n\n[VPC clusters](#attaching-storage-to-vpc-clusters)\n\n### Attaching storage to Classic Infrastructure clusters\n\n#### Install or migrate to Helm version 3 on cluster\n\nFor reference, instructions to install Helm v3 are [here](https://cloud.ibm.com/docs/openshift?topic=openshift-helm#install_v3).\n\n#### Install IBM Cloud Block Storage Attacher Plug-in\n\nFor reference, full documentation on IBM Cloud Block Storage Attacher Plug-in is [here](https://cloud.ibm.com/docs/openshift?topic=openshift-utilities#block_storage_attacher).\n\nIssue these commands:\n\n    oc project kube-system\n    helm repo update\n    helm install block-attacher iks-charts/ibm-block-storage-attacher\n    oc get pod -n kube-system -o wide | grep attacher\n    oc get storageclasses | grep attacher\n\n#### Attach newly created block storage to worker nodes\n\nFor reference, full documentation on how to attach raw block storage to worker nodes is [here](https://cloud.ibm.com/docs/openshift?topic=openshift-utilities#attach_block).\n\nList storage volumes\n\n    ibmcloud sl block volume-list\n\nIssue these two commands for each storage volume to get necessary information to create PVs below:\n\n    ibmcloud sl block access-list <volume_id>\n    ibmcloud sl block volume-detail <volume_id>\n\nCreate a PV to attach each storage block to a worker:\n\n    cat <<EOF | oc create -f -\n    apiVersion: v1\n    kind: PersistentVolume\n    metadata:\n    name: <pv_name>\n    annotations:\n      ibm.io/iqn: \"<IQN_hostname>\"\n      ibm.io/username: \"<username>\"\n      ibm.io/password: \"<password>\"\n      ibm.io/targetip: \"<targetIP>\"\n      ibm.io/lunid: \"<lunID>\"\n      ibm.io/nodeip: \"<private_worker_IP>\"\n      ibm.io/volID: \"<volume_ID>\"\n    spec:\n    capacity:\n      storage: <size>\n    accessModes:\n      - ReadWriteOnce\n    hostPath:\n      path: /\n    storageClassName: ibmc-block-attacher\n    EOF\n\nFor example:\n\n    cat <<EOF | oc create -f -\n    apiVersion: v1\n    kind: PersistentVolume\n    metadata:\n    name: px-pv-240\n    annotations:\n      ibm.io/iqn: \"iqn.2021-02.com.ibm:ibm02su2270065-i168011233\"\n      ibm.io/username: \"IBM02SU2270065-I168011233\"\n      ibm.io/password: \"xxxxxxxxxxxxx\"\n      ibm.io/targetip: \"161.26.110.196\"\n      ibm.io/lunid: \"0\"\n      ibm.io/nodeip: \"10.185.227.240\"\n      ibm.io/volID: \"214099980\"\n    spec:\n    capacity:\n      storage: 600Gi\n    accessModes:\n      - ReadWriteOnce\n    hostPath:\n      path: /\n    storageClassName: ibmc-block-attacher\n    EOF\n\nTo list and describe pv:\n\n    oc get pv\n    oc describe pv <pv_name>\n\nIf attached correctly, you will see `ibm.io/attachstatus: attached`\n\n    Name:            px-pv-238\n    Labels:          <none>\n    Annotations:     ibm.io/attachstatus: attached\n                     ibm.io/dm: /dev/dm-1\n                     ibm.io/iqn: iqn.2021-02.com.ibm:ibm02su2270065-i168011229\n                     ibm.io/lunid: 1\n                     ibm.io/mpath: 3600a09803830566f4724516532536b52\n                     ibm.io/nodeip: 10.185.227.238\n                     ibm.io/password: eRdwGTYSBbCZh444\n                     ibm.io/targetip: 161.26.110.204\n                     ibm.io/username: IBM02SU2270065-I168011229\n                     ibm.io/volID: 214100004\n    Finalizers:      [kubernetes.io/pv-protection]\n    StorageClass:    ibmc-block-attacher\n    Status:          Available\n    Claim:           \n    Reclaim Policy:  Retain\n    Access Modes:    RWO\n    VolumeMode:      Filesystem\n    Capacity:        600Gi\n    Node Affinity:   <none>\n    Message:         \n    Source:\n        Type:          HostPath (bare host directory volume)\n        Path:          /\n        HostPathType:  \n    Events:            <none>\n\n[Proceed to step 4](#4.-install-portworx-on-cluster)\n\n### Attaching storage to VPC clusters\n\nRetrieve IAM token, resource group and cluster.  Set environment variables to save typing in later commands.\n\n    IAM_TOKEN=$(ibmcloud iam oauth-tokens --output json | jq -r '.iam_token')\n    RESOURCE_GROUP=$(ibmcloud target --output json | jq -r '.resource_group.guid')\n    CLUSTER=<clustername>\n\nList storage volumes:\n\n\n    ibmcloud sl block volume-list\n\nGet worker details:\n\n    ibmcloud oc workers --cluster $CLUSTER\n\nUsing the id of the desired worker node and the storage volume, use CLI command to attach:\n\n    ibmcloud ks storage attachment create --cluster $CLUSTER --volume <volume_id> --worker <worker_id>\n\nFor Example:\n\n    ibmcloud ks storage attachment create --cluster $CLUSTER --volume 214099980 --worker kube-c0mktgsd0b2lo0bik8q0-cp4dpartner-default-000001f6\n    ibmcloud ks storage attachment create --cluster $CLUSTER --volume 214100004 --worker kube-c0mktgsd0b2lo0bik8q0-cp4dpartner-default-00000281\n    ibmcloud ks storage attachment create --cluster $CLUSTER --volume 214100042 --worker kube-c0mktgsd0b2lo0bik8q0-cp4dpartner-default-00000314 \n\nVerify attachments:\n\n    curl -X GET \"https://containers.cloud.ibm.com/v2/storage/getAttachments?cluster=$CLUSTER&worker=<worker_ID>\" --header \"X-Auth-Resource-Group-ID: $RESOURCE_GROUP\" --header \"Authorization: $IAM_TOKEN\"\n\n    curl -X GET \"https://containers.cloud.ibm.com/v2/storage/getAttachments?cluster=$CLUSTER&worker=kube-bvc11mbw0n2a9mccenlg-timropwxvpc-default-000002a7\" --header \"X-Auth-Resource-Group-ID: $RESOURCE_GROUP\" --header \"Authorization: $IAM_TOKEN\" {\"volume_attachments\":[{\"id\":\"0757-1e3db03e-e9bb-44ba-b423-14570a3511e6\",\"volume\":{\"name\":\"timro-pwx-vol01\",\"id\":\"r014-aef666d3-5072-4900-a1b2-23a69cb3f96b\"},\"device\":{\"id\":\"0757-1e3db03e-e9bb-44ba-b423-14570a3511e6-8xbd9\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"data\"},{\"id\":\"0757-6a52d8da-3192-4ac9-b78d-3fa3e402be4f\",\"volume\":{\"name\":\"gab-wistful-stimulate-nutmeg\",\"id\":\"r014-54a8d1d0-b679-4c7e-bec6-b91deb30e98f\"},\"device\":{\"id\":\"0757-6a52d8da-3192-4ac9-b78d-3fa3e402be4f-x5rql\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"boot\"}]}\n\n## 4. Install Portworx on cluster\n\n### Create an API Key\n\nFor reference, full documentation on how to create an API key is [here](https://cloud.ibm.com/docs/account?topic=account-userapikey)\n\nIMPORTANT: Ensure user creating API Key has sufficient permissions to work with cluster. They must be assigned the `Editor` platform role and the `Manager` service access role for `IBM Cloud Kubernetes Service`. For more information, see [User access permissions](https://cloud.ibm.com/docs/containers?topic=containers-access_reference).\n\nNOTE: Portworx won't install using a service id api key.\n\n- Select \"Manage\" -> \"Access (IAM)\" from IBM Cloud Console top menu\n- Select “API Keys” -> \"Create\"\n- Save the value of this key\n\n### Copy pull secrets\n\nCopy from `default` namespace to `kube-system` and associate with service account  (to use KVDB)\n\n    oc get secret all-icr-io -n default -o yaml | sed 's/default/kube-system/g' | oc create -n kube-system -f -\n    oc patch -n kube-system serviceaccount/default --type='json' -p='[{\"op\":\"add\",\"path\":\"/imagePullSecrets/-\",\"value\":{\"name\":\"all-icr-io\"}}]'\n    oc describe serviceaccount default -n kube-system\n\n### Use Public Catalog to install Portworx Enterprise\n\nFor reference, full documentation on how to install Portworx Enterprise is [here](https://cloud.ibm.com/docs/openshift?topic=openshift-portworx#install_portworx)\n\nFor non-production scenario, skip encryption. Choose to use in-cluster KVDB for simplicity.\n\nOpen the [Portworx catalog tile](https://cloud.ibm.com/catalog/services/portworx-enterprise):\n\n- Select Datacenter\n- Choose Pricing plan. For non production scenario, Disaster Recovery isn't needed\n- Enter a service name\n- Select resource group\n- Provide API key created in step 7 which will produce list of available clusters\n- Choose the target cluster\n- Under `tags` provide the cluster name. Do not provide any other tags.\n- Enter a Portworx cluster name\n- Select \"portworx KVDB\" in the pulldown for metadata key-value store\n- Select \"Kubernetes secret\" as the secret_type\n\n![portworx options](./images/portworx_options.png)\n\nAfter the service shows as active in the IBM Cloud resource view, verify the deployment:\n\n    kubectl get pods -n kube-system | grep 'portworx\\|stork'\n\nThis should display something like the following:\n\n    portworx-647c5                            1/1     Running     0          9m33s\n    portworx-api-h7dnr                        1/1     Running     0          9m33s\n    portworx-api-ndpxb                        1/1     Running     0          9m33s\n    portworx-api-srnjk                        1/1     Running     0          9m33s\n    portworx-gzgqc                            1/1     Running     0          9m33s\n    portworx-pvc-controller-b8c88b4d7-6rnq6   1/1     Running     0          9m33s\n    portworx-pvc-controller-b8c88b4d7-9bfxk   1/1     Running     0          9m33s\n    portworx-pvc-controller-b8c88b4d7-nqqpr   1/1     Running     0          9m33s\n    portworx-vxphk                            1/1     Running     0          9m33s\n    stork-6f74dcf5fc-mxwxb                    1/1     Running     0          9m33s\n    stork-6f74dcf5fc-svnrl                    1/1     Running     0          9m33s\n    stork-6f74dcf5fc-z9qlc                    1/1     Running     0          9m33s\n    stork-scheduler-7d755b5475-grzr2          1/1     Running     0          9m33s\n    stork-scheduler-7d755b5475-nl25m          1/1     Running     0          9m33s\n    stork-scheduler-7d755b5475-trhhb          1/1     Running     0          9m33s\n\nUsing one of the portworx pods, check the status of the storage cluster\n\n    kubectl exec portworx-647c5 -it -n kube-system -- /opt/pwx/bin/pxctl status\n\nThis should produce output like:\n\n    Status: PX is operational\n    License: PX-Enterprise IBM Cloud (expires in 1201 days)\n    Node ID: 5d65ce5b-1333-4b0c-b469-ccf7df1ce94a\n      IP: 172.26.0.10 \n      Local Storage Pool: 1 pool\n      POOL    IO_PRIORITY  RAID_LEVEL  USABLE    USED     STATUS  ZONE      REGION\n      0       LOW          raid0       400 GiB   18 GiB   Online  us-east-1 us-east\n      Local Storage Devices: 1 device\n      Device  Path      Media Type               Size     Last-Scan\n      0:1     /dev/vdd  STORAGE_MEDIUM_MAGNETIC  400 GiB  18 Dec 20 04:43 UTC\n      * Internal kvdb on this node is sharing this storage device /dev/vdd  to store its data.\n      total   -         400 GiB\n      Cache Devices:\n        * No cache devices\n    Cluster Summary\n      Cluster ID: pwx-iaf\n      Cluster UUID: 45fc03a8-7e82-497d-bc2a-0844dca1459f\n      Scheduler: kubernetes\n      Nodes: 3 node(s) with storage (3 online)\n      IP           ID                                   SchedulerNodeName  StorageNode  Used    Capacity  Status  StorageStatus Version         Kernel                      OS\n      172.26.0.9   f96e278c-fd06-42a8-9684-0d91bc0bde9c 172.26.0.9         Yes          18 GiB  400 GiB   Online  Up            2.6.1.6-3409af2 3.10.0-1160.6.1.el7.x86_64  Red Hat\n      172.26.0.10  5d65ce5b-1333-4b0c-b469-ccf7df1ce94a 172.26.0.10        Yes          18 GiB  400 GiB   Online  Up (This node 2.6.1.6-3409af2 3.10.0-1160.6.1.el7.x86_64  Red Hat\n      172.26.0.11  1b56ec6c-6dcd-4807-a9cd-cf1ae12e7635 172.26.0.11        Yes          18 GiB  400 GiB   Online  Up            2.6.1.6-3409af2 3.10.0-1160.6.1.el7.x86_64  Red Hat\n      Warnings: \n          WARNING: Internal Kvdb is not using dedicated drive on nodes [172.26.0.11 172.26.0.9 172.26.0.10]. This configuration is not recommended for production clusters.\n    Global Storage Pool\n      Total Used      :  53 GiB\n      Total Capacity  :  1.2 TiB\n\nTo review classificiation:\n\n    kubectl exec -it <portworx_pod> -n kube-system -- /opt/pwx/bin/pxctl cluster provision-status\n\nTest by creating a PVC - create a yaml file with a read-write-many claim:\n\n    cat <<EOF | oc create -f -\n    kind: PersistentVolumeClaim\n    apiVersion: v1\n    metadata:\n      name: mypvc\n    spec:\n      accessModes:\n      - ReadWriteMany\n      resources:\n        requests:\n          storage: 10G\n      storageClassName: portworx-shared-sc\n    EOF\n\n## 5. Create Portworx storage class definitions for Cloud Pak for Data\n\nCloud Pak for Data requires certain storage classes. To define them manually, go [here](https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_latest/cpd/install/portworx-storage-classes.html) to see the definitions.\n\nFor example `portworx-shared-gp3` is defined by:\n\n    cat <<EOF | oc create -f -\n    apiVersion: storage.k8s.io/v1\n    kind: StorageClass\n    metadata:\n      name: portworx-shared-gp3\n    parameters:\n      priority_io: high\n      repl: \"3\"\n      sharedv4: \"true\"\n      io_profile: db_remote\n      disable_io_profile_protection: \"1\"\n    allowVolumeExpansion: true\n    provisioner: kubernetes.io/portworx-volume\n    reclaimPolicy: Retain\n    volumeBindingMode: Immediate\n    EOF\n\n## Cleaning up and removing Portworx\n\nRun in the cluster:\n\n    curl -fsL https://install.portworx.com/px-wipe | bash\n\nRemove the resource from the IBM Cloud Console with delete, then deprovision the OpenShift Cluster\n","fileAbsolutePath":"/home/runner/work/ibm-enterprise-runbooks/ibm-enterprise-runbooks/src/pages/install/portworx-for-data/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","243517648","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550","817386451"]}