{"componentChunkName":"component---src-pages-programming-cf-index-mdx","path":"/programming/cf/","result":{"pageContext":{"frontmatter":{"title":"IBM Cloud Functions and IBM Event Streams"},"relativePagePath":"/programming/cf/index.mdx","titleType":"page","MdxNode":{"id":"f336bdf8-a77b-55d9-a2ec-bee6d1265980","children":[],"parent":"0f021fc0-946c-5ccb-8f22-ead4051ae686","internal":{"content":"---\ntitle: IBM Cloud Functions and IBM Event Streams\n\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\nSimple tutorial for reading IBM Event Streams (using Kafka) messages with IBM \nCloud Functions Python runtime.\n\n</PageDescription>\n\n## **Prerequisites**\n\n- An [IBM Cloud](https://cloud.ibm.com) account.\n\n## **Create you action**\n\nCreate an [IBM Cloud Functions Action](https://cloud.ibm.com/functions/actions) called `process-message`, that uses the **Python** runtime:\n\n1. Create a new Action\n    ![Create new IBM Cloud Functions Action](images/fn-actions-1.png)\n2. Name it `process-message`, use default package, select **Python** runtime then click **Create**:\n    ![Configure new IBM Cloud Functions Action](images/fn-actions-2.png)\n3. Put the following code inside the function:\n    ```python\n    def main(dict):\n        messages = dict.get('messages')\n\n        if messages is None or messages[0] is None:\n            return { 'error': \"Invalid arguments. Must include 'messages' JSON array\" }\n        try:\n            val = messages[0]['value']\n        except KeyError:\n            return { 'error': \"Invalid arguments. Must include 'messages' JSON array\" }\n\n        for i in range(0, len(messages)):\n            msg = messages[i]\n            print('Message received:', msg['value'])\n\n        return { 'messages': messages }\n    ```\n    ![Implement new IBM Cloud Functions Action](images/fn-actions-3.png)\n\n## **Provision IBM Event Streams**\n\n1. Provision an [IBM Event Streams](https://cloud.ibm.com/catalog/services/event-streams) instance. In this tutorial I'll call IBM Event Streams service instance `my-event-streams`:\n    ![Provision IBM Event Streams](images/es-provision.png)\n2. Once your service is created, create a new topic called `hello-world`:\n    ![Create new IBM Event Streams topic](images/es-create-topic.png)\n3. Create service credentials called `cloud-function`:\n    ![Create new IBM Event Streams service credentials](images/es-create-creds.png)\n4. Save the `kafka_brokers_sasl`, `apikey` and `kafka_admin_url` values from the new credentials for future steps.\n\n## **Create a new Trigger for incoming messages on IBM Event Streams**\n\nCreate an [IBM Cloud Functions Trigger](https://cloud.ibm.com/functions/triggers) called `new-message-trigger`, that reacts to incoming Kafka messages on your `hello-world` topic of your `my-event-streams` IBM Event Streams instance:\n\n1. Create the new trigger\n    ![Create new IBM Cloud Functions Trigger](images/fn-triggers-1.png)\n2. Select `Trigger`\n    ![Select Trigger resource](images/fn-triggers-2.png)\n3. Select `Event Streams` trigger type:\n    ![Select Event Streams trigger type](images/fn-triggers-3.png)\n4. Set `new-message-trigger` as the name, `my-event-streams` as Event Streams instance, `hello-world` as the topic to watch:\n    ![Configure new IBM Cloud Functions Trigger](images/fn-triggers-4.png)\n5. Bind your newly created trigger to your existing `process-message` action:\n    ![Bind Trigger to Action](images/fn-triggers-5.png)\n    ![Bind Trigger to Action](images/fn-triggers-6.png)\n\n## **Test it out**\n\nNow let's try that our action is triggered by a new message on topic `hello-world`. Open a terminal for the next steps.\n\n1. Clone event streams sample python client:\n    ```sh\n    git clone https://github.com/ibm-messaging/event-streams-samples\n    cd event-streams-samples/kafka-python-console-sample\n    ```\n2. In `event-streams-samples/kafka-python-console-sample/app.py`, replace:\n    ```python\n    self.topic_name = 'kafka-python-console-sample-topic'\n    ```\n    With:\n    ```python\n    self.topic_name = 'hello-world'\n    ```\n3. Run producer (use the values you copied from your Event Streams credentials, check [how to set up environment config](https://github.com/ibm-messaging/event-streams-samples/blob/master/kafka-python-console-sample/docs/Local.md) for more information):\n    ```sh\n    export KAFKA_BROKERS=<KAFKA_BROKERS>\n    export KAFKA_ADMIN_URL=<KAFKA_ADMIN_URL>\n    export KAFKA_APIKEY=<KAFKA_APIKEY>\n    python3 app.py $KAFKA_BROKERS $KAFKA_ADMIN_URL $KAFKA_APIKEY ../../certs.pem -producer\n    ```\n4. Open the [IBM Cloud Functions Activations Dashboard](https://cloud.ibm.com/functions/dashboard), you should see your action activated at each new message.\n    ![IBM Cloud Functions activations](images/fn-activations-1.png)\n    - If you click on an activation hash, you should see the test messages in the function's output.\n    - **NOTE**: The first activation takes more time and may handle several messages at once, since the action is cold. See the [Docs](https://cloud.ibm.com/docs/openwhisk?topic=openwhisk-about) for more information.\n\n## **Enable logging with LogDNA**\n\n1. Provision a new [IBM Log Analysis with LogDNA](https://cloud.ibm.com/catalog/services/logdna) instance. \n    ![Provision LogDNA](images/logdna-1.png)\n2. In [IBM Cloud Logging Dashboard](https://cloud.ibm.com/observe/logging), click **Configure platform logs**. \n    ![Provision LogDNA](images/logdna-3.png)\n3. Enable your LogDNA instance to handle platform logs. \n    ![Provision LogDNA](images/logdna-4.png)\n\nThere you are, your LogDNA instance now retrieves logs from your IBM Cloud Functions activations!","type":"Mdx","contentDigest":"cdceb788e5b342caad5c45bc5077a14a","counter":877,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"IBM Cloud Functions and IBM Event Streams"},"exports":{},"rawBody":"---\ntitle: IBM Cloud Functions and IBM Event Streams\n\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\nSimple tutorial for reading IBM Event Streams (using Kafka) messages with IBM \nCloud Functions Python runtime.\n\n</PageDescription>\n\n## **Prerequisites**\n\n- An [IBM Cloud](https://cloud.ibm.com) account.\n\n## **Create you action**\n\nCreate an [IBM Cloud Functions Action](https://cloud.ibm.com/functions/actions) called `process-message`, that uses the **Python** runtime:\n\n1. Create a new Action\n    ![Create new IBM Cloud Functions Action](images/fn-actions-1.png)\n2. Name it `process-message`, use default package, select **Python** runtime then click **Create**:\n    ![Configure new IBM Cloud Functions Action](images/fn-actions-2.png)\n3. Put the following code inside the function:\n    ```python\n    def main(dict):\n        messages = dict.get('messages')\n\n        if messages is None or messages[0] is None:\n            return { 'error': \"Invalid arguments. Must include 'messages' JSON array\" }\n        try:\n            val = messages[0]['value']\n        except KeyError:\n            return { 'error': \"Invalid arguments. Must include 'messages' JSON array\" }\n\n        for i in range(0, len(messages)):\n            msg = messages[i]\n            print('Message received:', msg['value'])\n\n        return { 'messages': messages }\n    ```\n    ![Implement new IBM Cloud Functions Action](images/fn-actions-3.png)\n\n## **Provision IBM Event Streams**\n\n1. Provision an [IBM Event Streams](https://cloud.ibm.com/catalog/services/event-streams) instance. In this tutorial I'll call IBM Event Streams service instance `my-event-streams`:\n    ![Provision IBM Event Streams](images/es-provision.png)\n2. Once your service is created, create a new topic called `hello-world`:\n    ![Create new IBM Event Streams topic](images/es-create-topic.png)\n3. Create service credentials called `cloud-function`:\n    ![Create new IBM Event Streams service credentials](images/es-create-creds.png)\n4. Save the `kafka_brokers_sasl`, `apikey` and `kafka_admin_url` values from the new credentials for future steps.\n\n## **Create a new Trigger for incoming messages on IBM Event Streams**\n\nCreate an [IBM Cloud Functions Trigger](https://cloud.ibm.com/functions/triggers) called `new-message-trigger`, that reacts to incoming Kafka messages on your `hello-world` topic of your `my-event-streams` IBM Event Streams instance:\n\n1. Create the new trigger\n    ![Create new IBM Cloud Functions Trigger](images/fn-triggers-1.png)\n2. Select `Trigger`\n    ![Select Trigger resource](images/fn-triggers-2.png)\n3. Select `Event Streams` trigger type:\n    ![Select Event Streams trigger type](images/fn-triggers-3.png)\n4. Set `new-message-trigger` as the name, `my-event-streams` as Event Streams instance, `hello-world` as the topic to watch:\n    ![Configure new IBM Cloud Functions Trigger](images/fn-triggers-4.png)\n5. Bind your newly created trigger to your existing `process-message` action:\n    ![Bind Trigger to Action](images/fn-triggers-5.png)\n    ![Bind Trigger to Action](images/fn-triggers-6.png)\n\n## **Test it out**\n\nNow let's try that our action is triggered by a new message on topic `hello-world`. Open a terminal for the next steps.\n\n1. Clone event streams sample python client:\n    ```sh\n    git clone https://github.com/ibm-messaging/event-streams-samples\n    cd event-streams-samples/kafka-python-console-sample\n    ```\n2. In `event-streams-samples/kafka-python-console-sample/app.py`, replace:\n    ```python\n    self.topic_name = 'kafka-python-console-sample-topic'\n    ```\n    With:\n    ```python\n    self.topic_name = 'hello-world'\n    ```\n3. Run producer (use the values you copied from your Event Streams credentials, check [how to set up environment config](https://github.com/ibm-messaging/event-streams-samples/blob/master/kafka-python-console-sample/docs/Local.md) for more information):\n    ```sh\n    export KAFKA_BROKERS=<KAFKA_BROKERS>\n    export KAFKA_ADMIN_URL=<KAFKA_ADMIN_URL>\n    export KAFKA_APIKEY=<KAFKA_APIKEY>\n    python3 app.py $KAFKA_BROKERS $KAFKA_ADMIN_URL $KAFKA_APIKEY ../../certs.pem -producer\n    ```\n4. Open the [IBM Cloud Functions Activations Dashboard](https://cloud.ibm.com/functions/dashboard), you should see your action activated at each new message.\n    ![IBM Cloud Functions activations](images/fn-activations-1.png)\n    - If you click on an activation hash, you should see the test messages in the function's output.\n    - **NOTE**: The first activation takes more time and may handle several messages at once, since the action is cold. See the [Docs](https://cloud.ibm.com/docs/openwhisk?topic=openwhisk-about) for more information.\n\n## **Enable logging with LogDNA**\n\n1. Provision a new [IBM Log Analysis with LogDNA](https://cloud.ibm.com/catalog/services/logdna) instance. \n    ![Provision LogDNA](images/logdna-1.png)\n2. In [IBM Cloud Logging Dashboard](https://cloud.ibm.com/observe/logging), click **Configure platform logs**. \n    ![Provision LogDNA](images/logdna-3.png)\n3. Enable your LogDNA instance to handle platform logs. \n    ![Provision LogDNA](images/logdna-4.png)\n\nThere you are, your LogDNA instance now retrieves logs from your IBM Cloud Functions activations!","fileAbsolutePath":"/home/runner/work/ibm-enterprise-runbooks/ibm-enterprise-runbooks/src/pages/programming/cf/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","243517648","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550","817386451"]}