---
title: IBM Cloud Functions and IBM Event Streams

---

import Globals from 'gatsby-theme-carbon/src/templates/Globals';

<PageDescription>

Simple tutorial for reading IBM Event Streams (using Kafka) messages with IBM 
Cloud Functions Python runtime.

</PageDescription>

## **Prerequisites**

- An [IBM Cloud](https://cloud.ibm.com) account.

## **Create you action**

Create an [IBM Cloud Functions Action](https://cloud.ibm.com/functions/actions) called `process-message`, that uses the **Python** runtime:

1. Create a new Action
    ![Create new IBM Cloud Functions Action](images/fn-actions-1.png)
2. Name it `process-message`, use default package, select **Python** runtime then click **Create**:
    ![Configure new IBM Cloud Functions Action](images/fn-actions-2.png)
3. Put the following code inside the function:
    ```python
    def main(dict):
        messages = dict.get('messages')

        if messages is None or messages[0] is None:
            return { 'error': "Invalid arguments. Must include 'messages' JSON array" }
        try:
            val = messages[0]['value']
        except KeyError:
            return { 'error': "Invalid arguments. Must include 'messages' JSON array" }

        for i in range(0, len(messages)):
            msg = messages[i]
            print('Message received:', msg['value'])

        return { 'messages': messages }
    ```
    ![Implement new IBM Cloud Functions Action](images/fn-actions-3.png)

## **Provision IBM Event Streams**

1. Provision an [IBM Event Streams](https://cloud.ibm.com/catalog/services/event-streams) instance. In this tutorial I'll call IBM Event Streams service instance `my-event-streams`:
    ![Provision IBM Event Streams](images/es-provision.png)
2. Once your service is created, create a new topic called `hello-world`:
    ![Create new IBM Event Streams topic](images/es-create-topic.png)
3. Create service credentials called `cloud-function`:
    ![Create new IBM Event Streams service credentials](images/es-create-creds.png)
4. Save the `kafka_brokers_sasl`, `apikey` and `kafka_admin_url` values from the new credentials for future steps.

## **Create a new Trigger for incoming messages on IBM Event Streams**

Create an [IBM Cloud Functions Trigger](https://cloud.ibm.com/functions/triggers) called `new-message-trigger`, that reacts to incoming Kafka messages on your `hello-world` topic of your `my-event-streams` IBM Event Streams instance:

1. Create the new trigger
    ![Create new IBM Cloud Functions Trigger](images/fn-triggers-1.png)
2. Select `Trigger`
    ![Select Trigger resource](images/fn-triggers-2.png)
3. Select `Event Streams` trigger type:
    ![Select Event Streams trigger type](images/fn-triggers-3.png)
4. Set `new-message-trigger` as the name, `my-event-streams` as Event Streams instance, `hello-world` as the topic to watch:
    ![Configure new IBM Cloud Functions Trigger](images/fn-triggers-4.png)
5. Bind your newly created trigger to your existing `process-message` action:
    ![Bind Trigger to Action](images/fn-triggers-5.png)
    ![Bind Trigger to Action](images/fn-triggers-6.png)

## **Test it out**

Now let's try that our action is triggered by a new message on topic `hello-world`. Open a terminal for the next steps.

1. Clone event streams sample python client:
    ```sh
    git clone https://github.com/ibm-messaging/event-streams-samples
    cd event-streams-samples/kafka-python-console-sample
    ```
2. In `event-streams-samples/kafka-python-console-sample/app.py`, replace:
    ```python
    self.topic_name = 'kafka-python-console-sample-topic'
    ```
    With:
    ```python
    self.topic_name = 'hello-world'
    ```
3. Run producer (use the values you copied from your Event Streams credentials, check [how to set up environment config](https://github.com/ibm-messaging/event-streams-samples/blob/master/kafka-python-console-sample/docs/Local.md) for more information):
    ```sh
    export KAFKA_BROKERS=<KAFKA_BROKERS>
    export KAFKA_ADMIN_URL=<KAFKA_ADMIN_URL>
    export KAFKA_APIKEY=<KAFKA_APIKEY>
    python3 app.py $KAFKA_BROKERS $KAFKA_ADMIN_URL $KAFKA_APIKEY ../../certs.pem -producer
    ```
4. Open the [IBM Cloud Functions Activations Dashboard](https://cloud.ibm.com/functions/dashboard), you should see your action activated at each new message.
    ![IBM Cloud Functions activations](images/fn-activations-1.png)
    - If you click on an activation hash, you should see the test messages in the function's output.
    - **NOTE**: The first activation takes more time and may handle several messages at once, since the action is cold. See the [Docs](https://cloud.ibm.com/docs/openwhisk?topic=openwhisk-about) for more information.

## **Enable logging with LogDNA**

1. Provision a new [IBM Log Analysis with LogDNA](https://cloud.ibm.com/catalog/services/logdna) instance. 
    ![Provision LogDNA](images/logdna-1.png)
2. In [IBM Cloud Logging Dashboard](https://cloud.ibm.com/observe/logging), click **Configure platform logs**. 
    ![Provision LogDNA](images/logdna-3.png)
3. Enable your LogDNA instance to handle platform logs. 
    ![Provision LogDNA](images/logdna-4.png)

There you are, your LogDNA instance now retrieves logs from your IBM Cloud Functions activations!